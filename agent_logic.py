"""
Agent Logic - ë…¸íŠ¸ë¶ì˜ Cell 4ì™€ Cell 5 ë¡œì§ì„ ê·¸ëŒ€ë¡œ ìœ ì§€
"""

from config import llm, emb, VECTORSTORE_DIR, LOG_DIR
from datetime import datetime

import os
import fitz  # PyMuPDF
import json
import re
from typing import Annotated, List, Literal, Dict, Any, Set, Optional
from typing_extensions import TypedDict
from copy import deepcopy

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage, AIMessage
from langgraph.prebuilt import create_react_agent
from langgraph.types import Command, Send
from langgraph.graph import MessagesState, StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver

# =========================
# ë¡œê¹… ìœ í‹¸ë¦¬í‹°
# =========================
def log_and_print(message: str, log_file: str = None):
    """
    ë©”ì‹œì§€ë¥¼ ì½˜ì†”ì— ì¶œë ¥í•˜ê³  ë™ì‹œì— ë¡œê·¸ íŒŒì¼ì— ì €ì¥
    
    Args:
        message: ì¶œë ¥í•  ë©”ì‹œì§€
        log_file: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ë¡œê·¸ íŒŒì¼ì— ì €ì¥ ì•ˆí•¨)
    """
    # ì½˜ì†”ì— ì¶œë ¥
    print(message)
    
    # ë¡œê·¸ íŒŒì¼ì— ì €ì¥
    if log_file:
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(message + '\n')


def create_log_file(patent_id: str, log_type: str = "preprocessing") -> str:
    """
    íŠ¹í—ˆë³„ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ìƒì„±
    
    Args:
        patent_id: íŠ¹í—ˆ ID (ì˜ˆ: US8526476)
        log_type: ë¡œê·¸ íƒ€ì… (preprocessing, chunking ë“±)
    
    Returns:
        ë¡œê·¸ íŒŒì¼ ì „ì²´ ê²½ë¡œ
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"{patent_id}_{log_type}_{timestamp}.log"
    return os.path.join(LOG_DIR, log_filename)


# =========================
# Vector Store ì €ì¥/ë¡œë“œ í•¨ìˆ˜
# =========================
def get_vectorstore_path(patent_id: str) -> str:
    """
    íŠ¹í—ˆ IDì— í•´ë‹¹í•˜ëŠ” vectorstore ì €ì¥ ê²½ë¡œ ë°˜í™˜
    
    Args:
        patent_id: íŠ¹í—ˆ ID (ì˜ˆ: US8526476)
    
    Returns:
        vectorstore ë””ë ‰í† ë¦¬ ê²½ë¡œ
    """
    return os.path.join(VECTORSTORE_DIR, patent_id)


def vectorstore_exists(patent_id: str) -> bool:
    """
    í•´ë‹¹ íŠ¹í—ˆì˜ vectorstoreê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    
    Args:
        patent_id: íŠ¹í—ˆ ID
    
    Returns:
        ì¡´ì¬ ì—¬ë¶€ (True/False)
    """
    path = get_vectorstore_path(patent_id)
    # FAISSëŠ” index.faissì™€ index.pkl íŒŒì¼ì´ ìˆì–´ì•¼ í•¨
    return os.path.exists(path) and os.path.exists(os.path.join(path, "index.faiss"))


def save_vectorstore(vectorstore: FAISS, patent_id: str) -> str:
    """
    Vectorstoreë¥¼ ë””ìŠ¤í¬ì— ì €ì¥
    
    Args:
        vectorstore: ì €ì¥í•  FAISS vectorstore
        patent_id: íŠ¹í—ˆ ID
    
    Returns:
        ì €ì¥ëœ ê²½ë¡œ
    """
    path = get_vectorstore_path(patent_id)
    os.makedirs(path, exist_ok=True)
    vectorstore.save_local(path)
    print(f"âœ… Vector store saved to: {path}")
    return path


def load_vectorstore(patent_id: str, embeddings) -> FAISS:
    """
    ë””ìŠ¤í¬ì—ì„œ Vectorstore ë¡œë“œ
    
    Args:
        patent_id: íŠ¹í—ˆ ID
        embeddings: Embedding ê°ì²´
    
    Returns:
        ë¡œë“œëœ FAISS vectorstore
    """
    path = get_vectorstore_path(patent_id)
    if not vectorstore_exists(patent_id):
        raise FileNotFoundError(f"Vectorstore not found for patent {patent_id} at {path}")
    
    vectorstore = FAISS.load_local(
        path,
        embeddings,
        allow_dangerous_deserialization=True  # FAISS ë¡œë“œì‹œ í•„ìš”
    )
    print(f"âœ… Vector store loaded from: {path}")
    return vectorstore


# =========================
# ê°„ë‹¨í•œ RAG ì±—ë´‡ í•¨ìˆ˜ (Tool ê¸°ë°˜)
# =========================
def simple_rag_chatbot(query: str, patent_id: str, chat_history: List[Dict] = None) -> str:
    """
    Tool ê¸°ë°˜ RAG ì±—ë´‡ - íŠ¹í—ˆ ë¬¸ì„œì— ëŒ€í•œ Q&A
    
    3ê°€ì§€ ê²€ìƒ‰ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹í—ˆ ë¬¸ì„œë¥¼ ë” ì •í™•í•˜ê²Œ ê²€ìƒ‰:
    - get_available_metadata: ì‚¬ìš© ê°€ëŠ¥í•œ ë©”íƒ€ë°ì´í„° í™•ì¸
    - search_by_metadata: íŠ¹ì • ì„¹ì…˜/ì²­êµ¬í•­ ê²€ìƒ‰
    - search_by_similarity: ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰
    
    Args:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        patent_id: íŠ¹í—ˆ ID
        chat_history: ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ (ì„ íƒì‚¬í•­)
    
    Returns:
        ì±—ë´‡ ì‘ë‹µ
    """
    try:
        global vectorstore, current_patent_id
        
        # Vectorstore ë¡œë“œ ë° global ë³€ìˆ˜ ì„¤ì •
        vectorstore = load_vectorstore(patent_id, emb)
        current_patent_id = patent_id
        
        # ê¸°ì¡´ íˆ´ë“¤ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        tools = [get_available_metadata, search_by_metadata, search_by_similarity]
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ êµ¬ì„±
        history_messages = []
        if chat_history:
            recent_history = chat_history[-6:]  # ìµœê·¼ 6ê°œë§Œ (3í„´)
            for msg in recent_history:
                if msg['role'] == 'user':
                    history_messages.append(HumanMessage(content=msg['content']))
                else:
                    history_messages.append(AIMessage(content=msg['content']))
        
        # ê°•í™”ëœ System í”„ë¡¬í”„íŠ¸
        system_prompt = f"""You are an expert patent analysis assistant with access to specialized search tools for Patent {patent_id}.

**YOUR MISSION:** Answer user questions accurately by intelligently using the provided tools.

**AVAILABLE TOOLS (Use them!):**

1. **get_available_metadata** - Check patent structure FIRST
   - Use when: User asks "ì–´ë–¤ ì„¹ì…˜ì´ ìˆì–´?", "claimì´ ëª‡ ê°œì•¼?", "êµ¬ì¡°ê°€ ì–´ë–»ê²Œ ë¼?"
   - Returns: Available sections, claim numbers, metadata fields
   - Example: get_available_metadata(metadata_keys=["section", "claim_no"])

2. **search_by_metadata** - Get specific sections/claims (STRUCTURAL search)
   - Use when: User wants specific claim numbers or entire sections
   - Examples:
     * "Claim 1 ë³´ì—¬ì¤˜" â†’ search_by_metadata(query="", filters="metadata['claim_no'] == 1")
     * "ABSTRACT ì„¹ì…˜ ë‚´ìš©" â†’ search_by_metadata(query="", filters="metadata['section'] == 'ABSTRACT'")
     * "ëª¨ë“  ë…ë¦½í•­" â†’ search_by_metadata(query="", filters="metadata['independent'] == True")
   - âš ï¸ Always use empty query "" for structural searches!

3. **search_by_similarity** - Semantic/concept search (MEANING search)
   - Use when: User asks conceptual questions
   - Examples:
     * "í˜ì‹  í¬ì¸íŠ¸ê°€ ë­ì•¼?" â†’ search_by_similarity(query="innovation points advantages", k=10)
     * "ê¸°ìˆ ì  ì›ë¦¬ ì„¤ëª…í•´ì¤˜" â†’ search_by_similarity(query="technical principle mechanism how it works", k=10)
     * "ì–´ë–¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ë‚˜?" â†’ search_by_similarity(query="problem solved background issues", k=10)
   - Use descriptive English keywords for better search results

**CRITICAL DECISION TREE:**

Question type â†’ Tool to use:
- "Claim X ë³´ì—¬ì¤˜" / "ì²­êµ¬í•­ X" â†’ search_by_metadata
- "ABSTRACT ì„¹ì…˜" / "íŠ¹ì • ì„¹ì…˜" â†’ search_by_metadata  
- "í˜ì‹ ", "ì¥ì ", "ì›ë¦¬", "ë°©ë²•", "íŠ¹ì§•" â†’ search_by_similarity
- "ì–´ë–¤ ì„¹ì…˜?", "ëª‡ ê°œ claim?" â†’ get_available_metadata
- Complex questions â†’ Use multiple tools sequentially

**ENHANCED GUIDELINES:**

1. **ALWAYS use tools** - Don't try to answer without searching!
2. **For claim questions:**
   - First: get_available_metadata(metadata_keys=["claim_no"]) to see available claims
   - Then: search_by_metadata with appropriate filter
3. **For concept questions:**
   - Use search_by_similarity with descriptive keywords
   - Search in English for better results (e.g., "innovation advantages benefits")
4. **Answer in Korean** - User's language
5. **Be specific** - Cite claim numbers, section names
6. **If not found** - Say "í•´ë‹¹ ë‚´ìš©ì´ ë¬¸ì„œì—ì„œ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
7. **Use multiple tools** - For complex questions, search multiple times

**EXAMPLE WORKFLOW:**

User: "ì´ íŠ¹í—ˆì˜ í˜ì‹  í¬ì¸íŠ¸ë¥¼ ì•Œë ¤ì¤˜"
â†’ Action: search_by_similarity(query="innovation points novel features advantages benefits", k=10)
â†’ Then synthesize and answer in Korean

User: "Claim 1 ë‚´ìš© ì•Œë ¤ì¤˜"  
â†’ Action: search_by_metadata(query="", filters="metadata['claim_no'] == 1")
â†’ Then present the claim content

**Remember:** Search FIRST, then answer based on results!"""

        # ReAct Agent ìƒì„±
        agent = create_react_agent(llm, tools)
        
        # ë©”ì‹œì§€ êµ¬ì„± (íˆìŠ¤í† ë¦¬ í¬í•¨)
        messages = [SystemMessage(content=system_prompt)] + history_messages + [HumanMessage(content=query)]
        
        # Agent ì‹¤í–‰
        state = {"messages": messages}
        result = agent.invoke(state)
        
        # ìµœì¢… ë‹µë³€ ì¶”ì¶œ (ë” ê°•ë ¥í•œ ì¶”ì¶œ ë¡œì§)
        if result and "messages" in result:
            # ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ ì°¾ê¸°
            for msg in reversed(result["messages"]):
                if isinstance(msg, AIMessage) and msg.content and msg.content.strip():
                    # Tool í˜¸ì¶œ ë©”ì‹œì§€ê°€ ì•„ë‹Œ ì‹¤ì œ ë‹µë³€ë§Œ ë°˜í™˜
                    # tool_callsê°€ ì—†ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µë§Œ ë°˜í™˜
                    if not hasattr(msg, 'tool_calls') or not msg.tool_calls:
                        return msg.content
            
            # fallback: ë§ˆì§€ë§‰ ë©”ì‹œì§€
            last_message = result["messages"][-1]
            if hasattr(last_message, 'content'):
                return last_message.content
            else:
                return str(last_message)
        else:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        print(f"âŒ Chatbot Error: {error_detail}")
        return f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\nê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ ë‹µë³€ì„ ì‹œë„í•©ë‹ˆë‹¤..."


# =========================
# 0) Helper Functions
# =========================

def load_pages_with_first_page_columns(pdf_path: str):
    """
    0ë²ˆ í˜ì´ì§€ë§Œ PyMuPDFë¡œ ì¢Œ/ìš° ì¹¼ëŸ¼ì„ ë¶„ë¦¬í•´ ì½ê³ ,
    1í˜ì´ì§€ ì´í›„ëŠ” PyPDFLoaderë¡œ ì½ì–´ Document ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜.
    """
    # 1) ìš°ì„  ì „ì²´ í˜ì´ì§€ ë©”íƒ€/í˜ì´ì§€ìˆ˜ íŒŒì•…ì„ ìœ„í•´ PyPDFLoader
    loader_pages = PyPDFLoader(pdf_path).load()  # pageë³„ Document
    assert len(loader_pages) >= 1, "PDFì— í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤."

    # 2) ì²« í˜ì´ì§€ë§Œ PyMuPDFë¡œ 2-ì¹¼ëŸ¼ ì¶”ì¶œ
    with fitz.open(pdf_path) as doc:
        p0 = doc[0]
        rect = p0.rect
        mid_x = rect.x0 + rect.width / 2.0

        left_rect  = fitz.Rect(rect.x0, rect.y0, mid_x,  rect.y1)
        right_rect = fitz.Rect(mid_x,  rect.y0, rect.x1, rect.y1)

        left_text  = p0.get_text("text", clip=left_rect) or ""
        right_text = p0.get_text("text", clip=right_rect) or ""

        # ì¹¼ëŸ¼ ìˆœì„œ: ë³´í†µ ì¢Œ -> ìš°ê°€ ìì—°ìŠ¤ëŸ¬ìš´ ì½ê¸° ìˆœì„œ
        first_text = (left_text.strip() + "\n" + right_text.strip()).strip()

    # 3) ì²« í˜ì´ì§€ Document ì¬êµ¬ì„± (metadata ë³´ì¡´ + ë³´ê°•)
    p0_meta = dict(loader_pages[0].metadata or {})
    p0_meta.update({
        "page": 0,
        "source": pdf_path,
        "column_split": "left|right"  # í›„ì²˜ë¦¬/ë””ë²„ê¹…ìš© ë§ˆì»¤
    })
    first_doc = Document(page_content=first_text, metadata=p0_meta)

    # 4) ë‚˜ë¨¸ì§€ í˜ì´ì§€ëŠ” ê¸°ì¡´ PyPDFLoader ê²°ê³¼(ë©”íƒ€ ìœ ì§€) í™œìš©
    rest_docs = []
    for d in loader_pages[1:]:
        m = dict(d.metadata or {})
        m.setdefault("source", pdf_path)
        rest_docs.append(Document(page_content=d.page_content, metadata=m))

    return [first_doc] + rest_docs


def to_langchain_document(patent_data: Dict[str, Any], source: str = "", log_file: str = None) -> List[Document]:
    """
    íŠ¹í—ˆ JSON ë°ì´í„°ë¥¼ LangChain Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    - ABSTRACT: 1ê°œ ë¬¸ì„œ (ë¶„í•  ì•ˆí•¨)
    - ê° ì„¹ì…˜ (CLAIMS ì œì™¸): ì²­í¬ë¡œ ë¶„í• 
    - CLAIMS: ê° ì²­êµ¬í•­ì„ ê°œë³„ ë¬¸ì„œë¡œ
    
    Args:
        patent_data: íŠ¹í—ˆ JSON ë°ì´í„° (metadata, sections, claims í¬í•¨)
        source: ì›ë³¸ íŒŒì¼ ê²½ë¡œ
        log_file: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
        
    Returns:
        List[Document]: ì²˜ë¦¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    metadata = patent_data.get("metadata", {})
    if source:
        metadata["source"] = source
    
    sections = patent_data.get("sections", {})
    claims = patent_data.get("claims", [])
    
    docs = []
    base_meta = deepcopy(metadata)
    
    # ì²­í¬ ìŠ¤í”Œë¦¬í„° ì„¤ì •
    desc_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500, 
        chunk_overlap=200, 
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    
    # 1) ABSTRACT ì²˜ë¦¬ (ë¶„í• í•˜ì§€ ì•ŠìŒ)
    abs_txt = sections.get("ABSTRACT", "") or ""
    if abs_txt.strip():
        meta = deepcopy(base_meta)
        meta.update({"section": "ABSTRACT", "granularity": "full"})
        docs.append(Document(page_content=abs_txt.strip(), metadata=meta))
    
    # 2) CLAIMS ì™¸ì˜ ëª¨ë“  ì„¹ì…˜ ì²˜ë¦¬ (desc_splitter ì‚¬ìš©)
    for sec_name, txt in sections.items():
        # ABSTRACTì™€ CLAIMSëŠ” ë³„ë„ ì²˜ë¦¬
        if sec_name in ["ABSTRACT", "CLAIMS"]:
            continue
        
        if not txt.strip():
            continue
        
        # desc_splitterë¡œ ì²­í¬ ë¶„í• 
        chunks = desc_splitter.create_documents([txt])
        
        for i, ch in enumerate(chunks):
            ch.metadata.update(deepcopy(base_meta))
            ch.metadata.update({
                "section": sec_name,
                "granularity": "chunk",
                "chunk_id": f"{sec_name}:{i}"
            })
            docs.append(ch)
    
    # 3) CLAIMS ì²˜ë¦¬ (ê° ì²­êµ¬í•­ì„ ê°œë³„ ë¬¸ì„œë¡œ)
    if claims:
        for claim_info in claims:
            claim_no = claim_info.get('claim_no')
            claim_text = claim_info.get('claim_text', '').strip()
            is_independent = claim_info.get('independent', False)
            
            if not claim_text:
                continue
            
            meta = deepcopy(base_meta)
            meta.update({
                "section": "CLAIMS",
                "granularity": "claim",
                "claim_no": claim_no,
                "independent": is_independent
            })
            docs.append(Document(page_content=claim_text, metadata=meta))
    

    # ============================================================
    # ë””ë²„ê¹…: ì²­í‚¹ ê²°ê³¼ ì¶œë ¥ (print + log)
    # ============================================================
    log_and_print("\n" + "="*80, log_file)
    log_and_print("ğŸ”ª ì²­í‚¹ ê²°ê³¼ - ëª¨ë“  ì²­í¬ ìƒì„¸ ë‚´ìš©", log_file)
    log_and_print("="*80 + "\n", log_file)
    log_and_print(f"ì´ ì²­í¬ ê°œìˆ˜: {len(docs)}ê°œ\n", log_file)
    
    for idx, doc in enumerate(docs, 1):
        section = doc.metadata.get('section', 'Unknown')
        granularity = doc.metadata.get('granularity', 'Unknown')
        chunk_length = len(doc.page_content)
        
        log_and_print("="*80, log_file)
        log_and_print(f"[ì²­í¬ {idx}/{len(docs)}]", log_file)
        log_and_print("="*80, log_file)
        log_and_print(f"ì„¹ì…˜: {section}", log_file)
        log_and_print(f"ì„¸ë¶„í™” ìˆ˜ì¤€: {granularity}", log_file)
        log_and_print(f"ê¸¸ì´: {chunk_length} ë¬¸ì", log_file)
        log_and_print(f"\nì „ì²´ ë‚´ìš©:", log_file)
        log_and_print(doc.page_content, log_file)
        log_and_print("\n" + "="*80 + "\n", log_file)
    
    log_and_print("\n" + "="*80, log_file)
    log_and_print("ğŸ“Š ì„¹ì…˜ë³„ ì²­í¬ í†µê³„", log_file)
    log_and_print("="*80, log_file)
    section_counts = {}
    for doc in docs:
        section = doc.metadata.get('section', 'Unknown')
        section_counts[section] = section_counts.get(section, 0) + 1
    
    for section, count in sorted(section_counts.items()):
        log_and_print(f"  {section}: {count}ê°œ ì²­í¬", log_file)
    
    log_and_print("\n" + "="*80, log_file)
    log_and_print("âœ… ì²­í‚¹ ê²°ê³¼ ì¶œë ¥ ì™„ë£Œ", log_file)
    log_and_print("="*80 + "\n", log_file)


    return docs


# =========================
# 1) LLM & Embeddings Setup
# =========================
# os.environ["OPENAI_API_KEY"] = "your-api-key-here"

# llm = ChatOpenAI(model="gpt-4o", temperature=0)
# emb = OpenAIEmbeddings()

# Global vectorstore (will be populated in preprocess_node)
vectorstore = None
all_claims = []  # (ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ - ë ˆê±°ì‹œ ì½”ë“œ)
current_patent_id = None  # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ íŠ¹í—ˆ ID

# =========================
# 2) Custom Retriever
# =========================
def custom_retrieve(query: str, k: int = 15) -> List[Document]:
    """
    ì»¤ìŠ¤í…€ retriever: similarity ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ kê°œ ë¬¸ì„œë¥¼ ê²€ìƒ‰
    (ëª¨ë“  ì²­êµ¬í•­ì„ ìë™ìœ¼ë¡œ í¬í•¨í•˜ì§€ ì•ŠìŒ)
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ê°’: 10)
        
    Returns:
        List[Document]: similarity ê¸°ë°˜ ìƒìœ„ kê°œ ë¬¸ì„œ
    """
    global vectorstore
    
    if vectorstore is None:
        return []
    
    # Similarity searchë¡œ kê°œ ê²€ìƒ‰
    results = vectorstore.similarity_search(query, k=k)
    
    return results


# =========================
# 3) RAG Tool: ë‘ ê°œì˜ ë¶„ë¦¬ëœ ê²€ìƒ‰ Tool
# =========================


@tool
def get_available_metadata(
    metadata_keys: Optional[List[str]] = None
) -> str:
    """
    íŠ¹í—ˆ ë¬¸ì„œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ë©”íƒ€ë°ì´í„° ê°’ë“¤ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ì´ íˆ´ì„ ì‚¬ìš©í•˜ì—¬:
    - íŠ¹í—ˆì— ì–´ë–¤ ì„¹ì…˜ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸
    - ëª‡ ë²ˆ claimê¹Œì§€ ìˆëŠ”ì§€ í™•ì¸
    - ê¸°íƒ€ ë©”íƒ€ë°ì´í„° í•„ë“œì™€ ê°’ë“¤ í™•ì¸
    
    Args:
        metadata_keys: ì¡°íšŒí•  ë©”íƒ€ë°ì´í„° í‚¤ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  í‚¤ ì¡°íšŒ)
                       ì˜ˆ: ["section", "claim_no", "granularity"]
    
    Returns:
        ì‚¬ìš© ê°€ëŠ¥í•œ ë©”íƒ€ë°ì´í„° ì •ë³´
    
    Examples:
        - get_available_metadata()  # ëª¨ë“  ë©”íƒ€ë°ì´í„° ì¡°íšŒ
        - get_available_metadata(["section"])  # ì„¹ì…˜ ëª©ë¡ë§Œ ì¡°íšŒ
        - get_available_metadata(["claim_no"])  # Claim ë²ˆí˜¸ ëª©ë¡ë§Œ ì¡°íšŒ
    """
    global vectorstore
    
    if vectorstore is None:
        return "Error: Patent document not yet preprocessed. Please wait for preprocessing to complete."
    
    # ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
    all_docs = vectorstore.similarity_search("", k=10000)
    
    # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
    metadata_values = {}
    
    # ê¸°ë³¸ì ìœ¼ë¡œ ì¡°íšŒí•  í‚¤ë“¤
    if metadata_keys is None:
        metadata_keys = ["section", "claim_no", "granularity", "independent"]
    
    for key in metadata_keys:
        values = set()
        for doc in all_docs:
            if key in doc.metadata:
                value = doc.metadata[key]
                if isinstance(value, list):
                    for item in value:
                        values.add(str(item))
                else:
                    values.add(str(value))
        
        if values:
            if key == "claim_no":
                try:
                    metadata_values[key] = sorted([int(v) for v in values if v.isdigit()])
                except:
                    metadata_values[key] = sorted(list(values))
            else:
                metadata_values[key] = sorted(list(values))
    
    # í¬ë§·íŒ…
    output = []
    output.append("=" * 80)
    output.append("ğŸ“Š AVAILABLE METADATA")
    output.append("=" * 80)
    output.append(f"\nì´ ë¬¸ì„œ ìˆ˜: {len(all_docs)}\n")
    
    for key, values in metadata_values.items():
        output.append(f"\nğŸ”¹ {key.upper()}")
        output.append("-" * 40)
        
        if key == "section":
            output.append(f"ì„¹ì…˜ ìˆ˜: {len(values)}")
            for v in values:
                count = sum(1 for doc in all_docs if doc.metadata.get('section') == v)
                output.append(f"  â€¢ {v} ({count} documents)")
        
        elif key == "claim_no":
            if values:
                output.append(f"Claim ë²”ìœ„: {min(values)} ~ {max(values)} (ì´ {len(values)}ê°œ)")
                independent_count = sum(
                    1 for doc in all_docs 
                    if doc.metadata.get('granularity') == 'claim' 
                    and doc.metadata.get('independent', False)
                )
                output.append(f"  â€¢ Independent Claims: {independent_count}ê°œ")
                output.append(f"  â€¢ Dependent Claims: {len(values) - independent_count}ê°œ")
        
        elif key == "granularity":
            for v in values:
                count = sum(1 for doc in all_docs if doc.metadata.get('granularity') == v)
                output.append(f"  â€¢ {v}: {count} documents")
        
        else:
            output.append(f"ê³ ìœ  ê°’ ìˆ˜: {len(values)}")
            if len(values) <= 20:
                for v in values:
                    count = sum(1 for doc in all_docs if str(doc.metadata.get(key)) == str(v))
                    output.append(f"  â€¢ {v} ({count} documents)")
    
    output.append("\n" + "=" * 80)
    output.append("ğŸ’¡ ì‚¬ìš© ë°©ë²•")
    output.append("=" * 80)
    output.append("\nìœ„ ë©”íƒ€ë°ì´í„° ê°’ë“¤ì„ search_by_metadata í•¨ìˆ˜ì˜ filters íŒŒë¼ë¯¸í„°ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    output.append("\ní˜¹ì€ search_by_similarity í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°œë…ì  ë˜ëŠ” ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë™ì¼í•œ ë‹¨ì–´ê°€ ì—†ì–´ë„ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë¬¸ë‹¨ì„ ì°¾ì•„ì¤ë‹ˆë‹¤.\ã…œ" \
    "ì§ˆì˜ì˜ ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì„¸ìš”.\n")
    # output.append("\nì˜ˆì‹œ:")
    # output.append('  search_by_metadata("ê²€ìƒ‰ì–´", filters={"section": "ABSTRACT"})')
    # output.append('  search_by_metadata("ê²€ìƒ‰ì–´", filters={"claim_no": 1})')
    
    return "\n".join(output)



@tool
def search_by_metadata(
    query: str,
    filters: str = None,
    k: int = 15
) -> str:
    """
    ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§ìœ¼ë¡œ íŠ¹í—ˆ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    Lambda í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ë™ì ìœ¼ë¡œ ë©”íƒ€ë°ì´í„°ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.
    
    âš ï¸ ì¤‘ìš” ì‚¬ìš© ê·œì¹™:
    - ì´ ë„êµ¬ëŠ” "íŠ¹ì • ì„¹ì…˜ì˜ ì „ì²´ ë‚´ìš©" ë˜ëŠ” "íŠ¹ì • claim ë²ˆí˜¸", "ì „ì²´ claim" ì„ ê°€ì ¸ì˜¬ ë•Œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - ê°œë…ì  ì§ˆë¬¸, ì˜ë¯¸ ê²€ìƒ‰, í‚¤ì›Œë“œ ê²€ìƒ‰ì—ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
    - ê°œë…ì  ì§ˆë¬¸(ì˜ˆ: "innovation points", "advantages", "problems solved", "how does X work")ì€ search_by_similarityë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    
    âš ï¸ ì‚¬ìš© ì „: ë¨¼ì € get_available_metadata íˆ´ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ë©”íƒ€ë°ì´í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”!
    
    ì˜¬ë°”ë¥¸ ì‚¬ìš© ì‹œê¸° (ONLY use when):
    - íŠ¹ì • ì„¹ì…˜ì˜ ì „ì²´ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ë•Œ (ì˜ˆ: ABSTRACT ì„¹ì…˜ ì „ì²´, CLAIMS ì„¹ì…˜ ì „ì²´)
    - íŠ¹ì • claim ë²ˆí˜¸ë¥¼ ì •í™•íˆ ê°€ì ¸ì˜¬ ë•Œ (ì˜ˆ: claim 1, claim 2-5)
    - ë…ë¦½í•­/ì¢…ì†í•­ì„ êµ¬ë¶„í•´ì„œ ê°€ì ¸ì˜¬ ë•Œ
    - êµ¬ì¡°ì ìœ¼ë¡œ ëª…í™•í•œ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§ì´ í•„ìš”í•œ ê²½ìš°
    
    ì˜ëª»ëœ ì‚¬ìš© ì˜ˆì‹œ (DO NOT use for):
    - "innovation points", "advantages", "problems solved" ê°™ì€ ê°œë…ì  ì§ˆë¬¸
    - "stepped edge", "etching conditions", "TMAH" ê°™ì€ í‚¤ì›Œë“œ ê²€ìƒ‰
    - "how does X work", "what is the mechanism" ê°™ì€ ì˜ë¯¸ì  ì§ˆë¬¸
    - íŠ¹ì • ì„¹ì…˜ ë‚´ì—ì„œ ê°œë…ì„ ê²€ìƒ‰í•˜ë ¤ëŠ” ê²½ìš° (ì´ ê²½ìš° search_by_similarity ì‚¬ìš©)
    
    Args:
        query: ê²€ìƒ‰ í‚¤ì›Œë“œ (í•„í„°ë§ëœ ê²°ê³¼ ë‚´ì—ì„œ ì •ë ¬ì— ì‚¬ìš©ë¨)
        filters: í•„í„°ë§ ì¡°ê±´ (Python í‘œí˜„ì‹ ë¬¸ìì—´)
                ì˜ˆ: "metadata['section'] == 'ABSTRACT'"
                    "metadata['claim_no'] == 1"
                    "metadata['section'] == 'CLAIMS' and metadata['independent'] == True"
        k: ë°˜í™˜í•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ê°’: 15)
    
    Returns:
        í•„í„°ë§ëœ íŠ¹í—ˆ ë‚´ìš©
    
    Examples:
        # 1ë‹¨ê³„: ì‚¬ìš© ê°€ëŠ¥í•œ ë©”íƒ€ë°ì´í„° í™•ì¸
        get_available_metadata(["section"])
        
        # 2ë‹¨ê³„: êµ¬ì¡°ì  ê²€ìƒ‰ (ì˜¬ë°”ë¥¸ ì‚¬ìš©)
        - search_by_metadata("", filters="metadata['section'] == 'ABSTRACT'")  # ABSTRACT ì„¹ì…˜ ì „ì²´ ê°€ì ¸ì˜¤ê¸°
        - search_by_metadata("", filters="metadata['section'] == 'CLAIMS'")  # CLAIMS ì„¹ì…˜ ì „ì²´ ê°€ì ¸ì˜¤ê¸°
        - search_by_metadata("", filters="metadata['claim_no'] == 1")  # Claim 1ë§Œ ê°€ì ¸ì˜¤ê¸°
        - search_by_metadata("", filters="metadata['section'] == 'CLAIMS' and metadata['independent'] == True")  # âš ï¸ë…ë¦½í•­ë§Œ ê°€ì ¸ì˜¤ê¸°âš ï¸
        - search_by_metadata("", filters="metadata['section'] == 'CLAIMS' and metadata['independent'] == False")  # âš ï¸ì¢…ì†í•­ë§Œ ê°€ì ¸ì˜¤ê¸°âš ï¸
    """
    global vectorstore
    
    if vectorstore is None:
        return "Error: Patent document not yet preprocessed. Please wait for preprocessing to complete."
    
    print(f"ğŸ” [METADATA FILTER] {filters}")
    
    # similarity_searchì— lambda í•„í„° ì ìš©
    if filters:
        try:
            # ë¬¸ìì—´ë¡œ ë°›ì€ filterë¥¼ lambda í•¨ìˆ˜ë¡œ ë³€í™˜
            filter_func = eval(f"lambda metadata: {filters}")
            
            # FAISSëŠ” filterë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ìˆ˜ë™ í•„í„°ë§
            # ì „ì²´ ë¬¸ì„œë¥¼ ê°€ì ¸ì™€ì„œ í•„í„°ë§ (similarity_searchê°€ ì•„ë‹Œ ì „ì²´ ë¬¸ì„œì—ì„œ)
            try:
                # FAISS vectorstoreì˜ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
                all_docs = list(vectorstore.docstore._dict.values())
            except:
                # docstore ì ‘ê·¼ ì‹¤íŒ¨ì‹œ fallback: ë¹ˆ ì¿¼ë¦¬ë¡œ ë§ì€ ìˆ˜ì˜ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
                all_docs = vectorstore.similarity_search("", k=100000)
            
            # í•„í„°ë§ ì ìš©
            filtered_docs = [doc for doc in all_docs if filter_func(doc.metadata)]
            
            print(f"   ğŸ“Š Total documents: {len(all_docs)}")
            print(f"   âœ… Filtered documents: {len(filtered_docs)}")
            
            # í•„í„°ë§ëœ ë¬¸ì„œê°€ ìˆìœ¼ë©´ ìƒìœ„ kê°œ ë°˜í™˜
            # ì¿¼ë¦¬ê°€ ìˆìœ¼ë©´ similarity ìˆœìœ¼ë¡œ, ì—†ìœ¼ë©´ ìˆœì„œëŒ€ë¡œ
            if filtered_docs and query.strip():
                # í•„í„°ë§ëœ ë¬¸ì„œë“¤ ì¤‘ì—ì„œ ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°
                # ì„ì‹œ vectorstore ìƒì„±í•˜ì—¬ ê²€ìƒ‰
                temp_vs = FAISS.from_documents(filtered_docs, vectorstore.embeddings)
                docs = temp_vs.similarity_search(query, k=min(k, len(filtered_docs)))
            else:
                docs = filtered_docs[:k]
            
        except Exception as e:
            print(f"âš ï¸ Filter evaluation failed: {e}")
            print(f"   Using empty result")
            docs = []
    else:
        docs = vectorstore.similarity_search(query, k=k)
    
    # ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
    if not docs:
        output = []
        output.append("=" * 80)
        output.append("âš ï¸  ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
        output.append("=" * 80)
        output.append(f"\nê²€ìƒ‰ì–´: {query}")
        output.append(f"í•„í„°: {filters}\n")
        output.append("ğŸ’¡ ì œì•ˆ:")
        output.append("1. get_available_metadata() íˆ´ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ë¨¼ì € í™•ì¸í•˜ì„¸ìš”")
        output.append("2. í•„í„° ì¡°ê±´ì„ ì™„í™”í•˜ê±°ë‚˜ ë‹¤ë¥¸ ë©”íƒ€ë°ì´í„°ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”")
        output.append("3. search_by_similarity() íˆ´ë¡œ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ì„ ì‹œë„í•´ë³´ì„¸ìš”")
        return "\n".join(output)
    
    # í¬ë§·íŒ… (ê¸°ì¡´ê³¼ ë™ì¼)
    output = []
    output.append("="*80)
    output.append(f"ğŸ” METADATA SEARCH: {query}")
    output.append(f"ğŸ¯ Filter: {filters}")
    output.append(f"ğŸ“Š Retrieved {len(docs)} documents")
    output.append("="*80 + "\n")
    
    # Claims ë¶€ë¶„
    claims = [d for d in docs if d.metadata.get('granularity') == 'claim']
    if claims:
        independent = [c for c in claims if c.metadata.get('independent', False)]
        dependent = [c for c in claims if not c.metadata.get('independent', False)]
        
        output.append("=" * 80)
        output.append("ğŸ“‹ CLAIMS")
        output.append("=" * 80)
        
        if independent:
            output.append("\nâ–  Independent Claims:")
            for claim in independent:
                claim_no = claim.metadata.get('claim_no', '?')
                output.append(f"\n[Claim {claim_no}]")
                content = claim.page_content
                content = re.sub(r'^\[CLAIM \d+\]\s+', '', content)
                output.append(content)
        
        if dependent:
            output.append("\n\nâ–  Dependent Claims:")
            for claim in dependent:
                claim_no = claim.metadata.get('claim_no', '?')
                refs = claim.metadata.get('references', [])
                if refs:
                    ref_str = ', '.join(map(str, refs))
                    output.append(f"\n[Claim {claim_no} - depends on: {ref_str}]")
                else:
                    output.append(f"\n[Claim {claim_no}]")
                content = claim.page_content
                content = re.sub(r'^\[CLAIM \d+\]\s+', '', content)
                output.append(content)
    
    # ê¸°íƒ€ ì„¹ì…˜
    other = [d for d in docs if d.metadata.get('granularity') != 'claim']
    if other:
        output.append("\n\n" + "=" * 80)
        output.append("ğŸ“„ RELEVANT SECTIONS")
        output.append("=" * 80)
        
        for doc in other:
            section = doc.metadata.get('section', 'Unknown')
            output.append(f"\n[{section}]")
            content = re.sub(r'^\[.*?\]\s*\n\n', '', doc.page_content)
            output.append(content)
            output.append("-" * 40)
    
    return "\n".join(output)




@tool  
def search_by_similarity(query: str, k: int = 15) -> str:
    """
    ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ íŠ¹í—ˆ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    ì˜ë¯¸ì ìœ¼ë¡œ ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì•„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    âš ï¸ ì¤‘ìš”: í˜„ì¬ íŠ¹í—ˆ(current_patent_id)ë¡œ ë¨¼ì € í•„í„°ë§í•œ í›„ similarity ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    ì˜¬ë°”ë¥¸ ì‚¬ìš© ì‹œê¸° (USE THIS FOR):
    - ê°œë…ì /ì˜ë¯¸ì  ì§ˆë¬¸: "innovation points", "problems solved", "advantages", "benefits"
    - í‚¤ì›Œë“œ ê²€ìƒ‰: "stepped edge", "etching conditions", "TMAH", "HCl etching"
    - ê¸°ìˆ ì  ë‚´ìš© ê²€ìƒ‰: "how does X work", "what is the mechanism", "implementation method"
    - íŠ¹ì • ê°œë…ì´ë‚˜ ê¸°ìˆ ì— ëŒ€í•œ ì¼ë°˜ì  ì§ˆë¬¸
    - ì„¹ì…˜ì„ ì§€ì •í•˜ì§€ ì•Šì€ ëª¨ë“  ê°œë…ì  ê²€ìƒ‰
    
    ì‚¬ìš©í•˜ì§€ ë§ì•„ì•¼ í•  ê²½ìš°:
    - íŠ¹ì • claim ë²ˆí˜¸ë¥¼ ì •í™•íˆ ê°€ì ¸ì˜¬ ë•Œ (ì˜ˆ: claim 1ë§Œ) â†’ search_by_metadata ì‚¬ìš©
    - íŠ¹ì • ì„¹ì…˜ì˜ ì „ì²´ ë‚´ìš©ë§Œ ê°€ì ¸ì˜¬ ë•Œ (ì˜ˆ: ABSTRACT ì „ì²´) â†’ search_by_metadata ì‚¬ìš©
    
    Args:
        query: ê²€ìƒ‰í•  ì§ˆë¬¸ì´ë‚˜ ê°œë… (ì˜ˆ: "innovation points of this patent", "stepped edge advantages", "etching conditions")
        k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ê°’: 15)
    
    Returns:
        ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ íŠ¹í—ˆ ë‚´ìš© (ìë™ìœ¼ë¡œ ê´€ë ¨ ì„¹ì…˜ê³¼ claims í¬í•¨)
    
    Examples:
        - search_by_similarity("innovation points of this patent, what problems solved, advantages")
        - search_by_similarity("stepped edge and superlattice, effective mass reduction")
        - search_by_similarity("HCl etching 840-860 C TMAH developer")
        - search_by_similarity("DEPENDENT claims with etching conditions TMAH HCl")
    """
    global vectorstore, current_patent_id
    
    if vectorstore is None:
        return "Error: Patent document not yet preprocessed. Please wait for preprocessing to complete."
    
    print(f"ğŸ” [SIMILARITY SEARCH]")
    print(f"   Query: {query}")
    print(f"   Current Patent ID: {current_patent_id}")
    
    # 1ë‹¨ê³„: patent_idë¡œ ë¨¼ì € í•„í„°ë§ (ìˆëŠ” ê²½ìš°)
    if current_patent_id:
        print(f"   âš¡ Filtering by patent_id: {current_patent_id}")
        # patent_idë¡œ í•„í„°ë§ëœ ë¬¸ì„œì—ì„œ similarity ê²€ìƒ‰
        filter_dict = {'patent_id': current_patent_id}
        try:
            docs = vectorstore.similarity_search(
                query,
                k=k,
                filter=filter_dict
            )
            print(f"   âœ… Filtered results: {len(docs)} documents")
        except Exception as e:
            # í•„í„°ë§ ì‹¤íŒ¨ì‹œ ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ fallback
            print(f"   âš ï¸ Filter failed, using regular search: {e}")
            docs = vectorstore.similarity_search(query, k=15)
    else:
        # patent_idê°€ ì—†ìœ¼ë©´ ì¼ë°˜ similarity ê²€ìƒ‰
        print(f"   â„¹ï¸ No patent_id filter, using regular similarity search")
        docs = vectorstore.similarity_search(query, k=15)
    
    # í¬ë§·íŒ…
    output = []
    output.append("="*80)
    output.append(f"ğŸ” SIMILARITY SEARCH: {query}")
    if current_patent_id:
        output.append(f"ğŸ¯ Filtered by Patent ID: {current_patent_id}")
    output.append(f"ğŸ“Š Retrieved {len(docs)} documents")
    output.append("="*80 + "\n")
    
    # Claims ë¶„ë¦¬
    claims = [d for d in docs if d.metadata.get('granularity') == 'claim']
    if claims:
        output.append("=" * 80)
        output.append("ğŸ“‹ RELEVANT CLAIMS")
        output.append("=" * 80)
        
        for claim in claims:
            claim_no = claim.metadata.get('claim_no', '?')
            claim_type = "Independent" if claim.metadata.get('independent') else "Dependent"
            output.append(f"\n[Claim {claim_no} - {claim_type}]")
            content = re.sub(r'^\[CLAIM \d+\]\s+', '', claim.page_content)
            output.append(content)
    
    # ê¸°íƒ€ ì„¹ì…˜
    other = [d for d in docs if d.metadata.get('granularity') != 'claim']
    if other:
        output.append("\n\n" + "=" * 80)
        output.append("ğŸ“„ RELEVANT SECTIONS")
        output.append("=" * 80)
        
        for doc in other:
            section = doc.metadata.get('section', 'Unknown')
            output.append(f"\n[{section}]")
            content = re.sub(r'^\[.*?\]\s*\n\n', '', doc.page_content)
            output.append(content)
    
    return "\n".join(output)


# =========================
# 3.5) New Tools for Horizontal Agent - Patent Comparison
# =========================

@tool
def generate_patent_search_query(abstract: str) -> str:
    """
    íŠ¹í—ˆ abstractë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬í•œ íŠ¹í—ˆë¥¼ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ ê²€ìƒ‰ì‹ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    LLMì„ ì‚¬ìš©í•˜ì—¬ abstractì—ì„œ í•µì‹¬ ê¸°ìˆ  í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³ ,
    Google Patentsì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìµœì í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        abstract: íŠ¹í—ˆì˜ abstract ì „ë¬¸
    
    Returns:
        Google Patents ê²€ìƒ‰ì— ìµœì í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬
    
    Example:
        abstract = "A semiconductor device with stepped edge..."
        query = generate_patent_search_query(abstract)
        # Returns: "semiconductor device stepped edge superlattice"
    """
    if not abstract or len(abstract.strip()) < 50:
        return "Error: Abstract is too short or empty. Please provide a valid abstract."


    
    prompt = f"""You are an expert in prior-art search and Google Patents query design.

    GOAL:
    - Your top priority is to FIND many relevant similar patents (high recall), not just a few extremely precise hits.
    - The query must work well in Google Patents and should avoid being too narrow.

    INTERNAL STRATEGY (do NOT include these steps in the output):
    1. From the abstract, identify:
    - (a) the DEVICE/COMPONENT type (e.g., transistor, memory, DRAM, LED)
    - (b) the CORE STRUCTURE or PROCESS (e.g., superlattice, stepped edge, STI recess, gate stack)
    - (c) the FUNCTIONAL EFFECT or PURPOSE (e.g., reduced leakage, improved reliability, phonon isolation).
    2. For each of (a), (b), (c), think of 1â€“2 common synonyms or alternative phrases used in patents.
    - Example: superlattice â‰ˆ "multiple quantum well" OR MQW
    3. Prefer general, widely-used technical terms over very niche or proprietary words.
    4. Combine these into a single query using:
    - OR between synonyms
    - AND (implicit by space) between main concepts

    OUTPUT FORMAT:
    - Return ONLY ONE final query string.
    - Use 3â€“6 conceptual elements (words or short phrases), not just 2.
    - Use parentheses and OR for synonyms.
    - Do NOT add any explanation.

    GOOD EXAMPLES:
    Abstract: "A semiconductor device with stepped edge for quantum wells..."
    Query: (stepped edge) (semiconductor OR "quantum well")

    Abstract: "A memory device using superlattice structures with phonon isolation..."
    Query: (superlattice OR "multiple quantum well" OR MQW) (memory OR storage) phonon

    Abstract: "A transistor with non-semiconductor monolayer barriers..."
    Query: (non-semiconductor monolayer OR "insulating monolayer") (transistor OR FET) barrier

    BAD EXAMPLES (too narrow or too many random words):
    âŒ "recessed active region stepped edge isolation superlattice non-semiconductor monolayer"
    âŒ "semiconductor device memory storage quantum well superlattice structure reliability efficiency"

    Abstract:
    {abstract}

    Now generate ONLY the final Google Patents search query string:"""


    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        search_query = response.content.strip()
        
        # Remove quotes if present at the beginning/end
        search_query = search_query.strip('"').strip("'")
        
        # Validate query complexity
        word_count = len(search_query.replace('(', ' ').replace(')', ' ').replace('"', ' ').split())
        
        print(f"âœ… Generated search query: {search_query}")
        print(f"   Query complexity: {word_count} keywords")
        
        # If too complex (>7 words excluding operators), warn and suggest simplification
        if word_count > 7:
            print(f"   âš ï¸  Query might be too specific. Consider simplifying if no results found.")
            # Extract first 4 meaningful words as fallback
            words = [w for w in search_query.split() if w not in ['OR', 'AND', '(', ')', '"']]
            if len(words) > 4:
                simpler_query = ' '.join(words[:4])
                print(f"   ğŸ’¡ Fallback query: {simpler_query}")
        
        return search_query
    
    except Exception as e:
        return f"Error generating search query: {str(e)}"



@tool
def refine_patent_search_query(
    abstract: str,
    previous_query: str,
    iteration: int = 1
) -> str:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ íŠ¹í—ˆ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì ì§„ì ìœ¼ë¡œ ê³ ë„í™”í•©ë‹ˆë‹¤.
    í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³ , refined_queryë¥¼ ë§Œë“¤ì–´ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        abstract: íŠ¹í—ˆ ì´ˆë¡ (ì¿¼ë¦¬ ìƒì„±ì— ì‚¬ìš©)
        previous_query: ì´ì „ ì¿¼ë¦¬ (ê°œì„  ëŒ€ìƒ)
        iteration: í˜„ì¬ iteration ë²ˆí˜¸ (ìµœëŒ€ 8)
    
    Returns:
        JSON string:
        {
            "success": true/false,
            "refined_query": "...",  ...
        }
    """
    print(f"\n{'='*80}")
    print(f"ğŸ¤– REFINE ITERATION {iteration}/8")
    print(f"{'='*80}")
    print(f"Previous Query: {previous_query}")
    
    if iteration > 8:
        return json.dumps({
            "error": True,
            "message": "Maximum iterations (8) reached",
            "previous_query": previous_query
        }, ensure_ascii=False)
    
    prompt = f"""You are a patent prior-art search expert specializing in Google Patents.

Your role:
- Refine and improve patent search queries to find the most relevant similar patents
- This is NOT just for failed searches - you improve ANY query to make it more sophisticated and effective

Input:
- abstract: The current patent's abstract
- previous_query: The query to be improved (may be initial or already refined)
- iteration: Current refinement iteration number (1-8)

Output:
- A refined query with feedback explaining the improvements made

-------------------------------------------------------------------------------
GENERAL GUIDELINES FOR QUERY REFINEMENT
-------------------------------------------------------------------------------

Your goal: Create progressively better queries that balance precision and recall.

Core Principles:
1. Focus on 3â€“6 core technical concepts (device type, key structure, process, material, principle)
2. Use standard patent terminology and technical language
3. Use AND to combine distinct concepts, OR for synonyms
4. Keep queries concise (5â€“12 keywords optimal)
5. Include physical/functional keywords when relevant (bandgap, interface, strain, leakage, quantum, etc.)

What to avoid:
- Too many AND constraints (over-constraining)
- Long literal phrases copied from abstract
- Non-technical stopwords (including, wherein, having, according to, etc.)
- Patent numbers or company names
- Overly specific numbers or measurements

HOW TO USE FEEDBACK FOR REFINEMENT:
- Each iteration builds on the previous query by applying constructive improvements
- Use the GENERAL GUIDELINES above to decide what to improve
- Consider the feedback from previous iterations to avoid repeating unsuccessful approaches
- Progressive refinement: start with core concepts, then expand/adjust based on results
- Balance precision and recall - don't over-constrain too early

REFINEMENT APPROACH:
1. Analyze the previous_query to identify strengths and weaknesses
2. Apply improvements from GENERAL GUIDELINES:
   - Optimize keyword selection (add/remove/replace terms)
   - Adjust synonym coverage (add OR alternatives)
   - Modify specificity level (broader or narrower)
   - Incorporate technical principles when relevant
   - Remove over-constraints if query is too narrow
3. Generate constructive feedback explaining the improvements
4. Output the refined query with clear rationale

Two main approaches to consider:

**Similar Approach**: Find patents with similar device architecture
- Extract: device type + key structure + key process/material
- Example: "(MOSFET OR transistor) AND (buried gate OR embedded gate) AND (high-k dielectric OR HfO2 OR gate oxide)"

**Base Technology Approach**: Find patents sharing underlying principles
- Identify: physical principle or engineering goal
- Generalize specific terms to broader concepts
- Example: "(semiconductor device) AND (bandgap engineering OR energy band modulation) AND (superlattice OR quantum well OR heterostructure)"

Choose or blend approaches based on the abstract and previous_query.

-------------------------------------------------------------------------------
FEEDBACK GUIDELINES
-------------------------------------------------------------------------------

Provide constructive feedback (2-4 sentences) explaining:
- What aspects of previous_query were good
- What specific improvements you're making
- Why these changes will help find more relevant patents

Focus on IMPROVEMENTS, not failures. Be constructive and specific.

Good feedback examples:
- "The previous query had good core concepts but was too narrow. I'm adding synonyms and broadening the structural terms to capture more related patents while maintaining focus."
- "Building on the solid foundation, I'm incorporating the underlying physical principle (bandgap engineering) to find patents that share the same technical approach even with different structures."
- "The query is well-structured. I'm fine-tuning by adding process-related terms and adjusting synonym coverage to improve recall without sacrificing precision."

-------------------------------------------------------------------------------
CONTEXT
-------------------------------------------------------------------------------

- Iteration: {iteration}/8
- Previous query: "{previous_query}"

PATENT ABSTRACT (truncated to ~500 chars):
{abstract[:500]}...

-------------------------------------------------------------------------------
OUTPUT FORMAT (JSON only, no extra text)
-------------------------------------------------------------------------------

{{
  "feedback": "Constructive feedback on improvements being made (2-4 sentences)",
  "refined_query": "The improved Google Patents search query"
}}

IMPORTANT: 
- refined_query MUST be different from previous_query (unless previous_query is already optimal)
- Focus on making queries progressively better, not just reacting to failures
- Each iteration should add sophistication and refinement
"""
    
    try:
        resp = llm.invoke([HumanMessage(content=prompt)])
        content = resp.content.strip()
        
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()
        
        analysis = json.loads(content)
        
        if "error" in analysis:
            return json.dumps({"error": True, "message": "LLM failed"}, ensure_ascii=False)
        
        refined_query = analysis.get("refined_query", "")
        feedback = analysis.get("feedback", "")        
        if not refined_query:
            return json.dumps({"error": True, "message": "No query generated"}, ensure_ascii=False)
        
        print(f"ğŸ’¡ Feedback: {feedback}")
        
        
        print(f"ğŸ” Refined Query: {refined_query}\n")
        
        result = {
            "success": True,
            "refined_query": refined_query,
            "feedback": feedback,
            "iteration": iteration,
            "previous_query": previous_query
        }
        
        print(f"âœ… Refined: {previous_query} â†’ {refined_query}")
        
        return json.dumps(result, ensure_ascii=False)
        
    except json.JSONDecodeError as e:
        return json.dumps({"error": True, "message": f"JSON error: {str(e)}"}, ensure_ascii=False)
    except Exception as e:
        return json.dumps({"error": True, "message": f"Error: {str(e)}"}, ensure_ascii=False)
    

@tool
def search_similar_patents_serpapi(search_query: str, num_results: int = 10) -> str:
    """
    SerpAPIë¥¼ í†µí•´ Google Patentsì—ì„œ ìœ ì‚¬í•œ íŠ¹í—ˆë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    ë‹¨ìˆœ ê²€ìƒ‰ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        search_query: Google Patents ê²€ìƒ‰ ì¿¼ë¦¬
        num_results: ë°˜í™˜í•  íŠ¹í—ˆ ê°œìˆ˜ (ê¸°ë³¸ê°’: 2, ìµœëŒ€ 10)
    
    Returns:
        ì„±ê³µ: íŠ¹í—ˆ ì •ë³´ (formatted string)
        ì‹¤íŒ¨: "NO_RESULTS_FOUND"
    """
    try:
        from serpapi import GoogleSearch
    except ImportError:
        return "Error: google-search-results package not installed"
    
    serpapi_key = os.getenv("SERPAPI_API_KEY")
    if not serpapi_key:
        return "Error: SERPAPI_API_KEY not set"
    
    num_results = min(max(1, num_results), 10)
    
    print(f"\nğŸ” Searching Google Patents...")
    print(f"   Query: {search_query}")
    print(f"   Results: {num_results}")
    
    try:
        params = {
            "engine": "google_patents",
            "q": search_query,
            "api_key": serpapi_key,
            "num": max(10, num_results)  # SerpAPI ê¶Œì¥ ìµœì†Œê°’ 10
        }
        
        search = GoogleSearch(params)
        results = search.get_dict()
        
        if "organic_results" not in results or not results["organic_results"]:
            print(f"   âŒ No results found\n")
            return "NO_RESULTS_FOUND"
        
        print(f"   âœ… Found {len(results['organic_results'])} patents\n")
        
        formatted = []
        formatted.append("="*80)
        formatted.append("âœ… SIMILAR PATENTS FOUND")
        formatted.append("="*80)
        formatted.append(f"\nQuery: {search_query}")
        formatted.append(f"Total Results: {len(results['organic_results'])}\n")
        
        for idx, patent in enumerate(results["organic_results"], 1):
            formatted.append(f"\n{'='*80}")
            formatted.append(f"Patent {idx}")
            formatted.append(f"{'='*80}")
            
            title = patent.get("title", "No title")
            patent_id = patent.get("patent_id", "Unknown")
            snippet = patent.get("snippet", "No snippet")
            pdf_link = patent.get("pdf", "")
            filing_date = patent.get("filing_date", "")
            assignee = patent.get("assignee", "")
            
            formatted.append(f"\nğŸ“Œ Title: {title}")
            formatted.append(f"ğŸ“„ Patent ID: {patent_id}")
            
            if assignee:
                formatted.append(f"ğŸ¢ Assignee: {assignee}")
            if filing_date:
                formatted.append(f"ğŸ“… Filing Date: {filing_date}")
            
            formatted.append(f"\nğŸ“ Summary:\n{snippet}")
            
            if pdf_link:
                formatted.append(f"\nğŸ”— PDF: {pdf_link}")
            
            formatted.append("")
        
        formatted.append("="*80)
        return "\n".join(formatted)
        
    except Exception as e:
        return f"SEARCH_ERROR: {str(e)}"
@tool
def search_similar_patents_serpapi(search_query: str, num_results: int = 2) -> str:
    """
    SerpAPIë¥¼ í†µí•´ Google Patentsì—ì„œ ìœ ì‚¬í•œ íŠ¹í—ˆë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    ë‹¨ìˆœ ê²€ìƒ‰ë§Œ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        search_query: Google Patents ê²€ìƒ‰ ì¿¼ë¦¬
        num_results: ë°˜í™˜í•  íŠ¹í—ˆ ê°œìˆ˜ (ê¸°ë³¸ê°’: 2, ìµœëŒ€ 10)
    
    Returns:
        ê²€ìƒ‰ ì„±ê³µ: íŠ¹í—ˆ ì •ë³´ (formatted string)
        ê²€ìƒ‰ ì‹¤íŒ¨: "NO_RESULTS_FOUND" ë¬¸ìì—´
    
    Example:
        results = search_similar_patents_serpapi("semiconductor device", num_results=2)
    """
    try:
        from serpapi import GoogleSearch
    except ImportError:
        return ("Error: 'google-search-results' package not installed. "
                "Please install it with: pip install google-search-results")
    
    # SerpAPI API í‚¤ í™•ì¸
    serpapi_key = os.getenv("SERPAPI_API_KEY")
    if not serpapi_key:
        return ("Error: SERPAPI_API_KEY environment variable not set. "
                "Please set it with your SerpAPI key from https://serpapi.com/")
    
    # num_results ì œí•œ
    num_results = min(max(1, num_results), 10)
    
    print(f"\nğŸ” Searching Google Patents via SerpAPI...")
    print(f"   Query: {search_query}")
    print(f"   Number of results: {num_results}")
    
    try:
        # SerpAPI Google Patents ê²€ìƒ‰
        params = {
            "engine": "google_patents",
            "q": search_query,
            "api_key": serpapi_key,
            "num": max(10, num_results)  # SerpAPI ê¶Œì¥ ìµœì†Œê°’ 10
        }
        
        search = GoogleSearch(params)
        results = search.get_dict()
        
        # ê²°ê³¼ í™•ì¸
        if "organic_results" not in results or not results["organic_results"]:
            print(f"   âŒ No results found")
            return "NO_RESULTS_FOUND"
        
        # ì„±ê³µì ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì°¾ì€ ê²½ìš°
        print(f"   âœ… Found {len(results['organic_results'])} results!")
        
        formatted_results = []
        formatted_results.append("=" * 80)
        formatted_results.append("âœ… SIMILAR PATENTS FOUND")
        formatted_results.append("=" * 80)
        formatted_results.append(f"\nQuery: {search_query}")
        formatted_results.append(f"Total Results: {len(results['organic_results'])}\n")
        
        for idx, patent in enumerate(results["organic_results"], 1):
            formatted_results.append(f"\n{'='*80}")
            formatted_results.append(f"Patent {idx}")
            formatted_results.append(f"{'='*80}")
            
            title = patent.get("title", "No title")
            patent_id = patent.get("patent_id", "Unknown")
            snippet = patent.get("snippet", "No snippet available")
            pdf_link = patent.get("pdf", "")
            filing_date = patent.get("filing_date", "")
            priority_date = patent.get("priority_date", "")
            grant_date = patent.get("grant_date", "")
            inventor = patent.get("inventor", "")
            assignee = patent.get("assignee", "")
            
            formatted_results.append(f"\nğŸ“Œ Title: {title}")
            formatted_results.append(f"ğŸ“„ Patent ID: {patent_id}")
            
            if assignee:
                formatted_results.append(f"ğŸ¢ Assignee: {assignee}")
            if inventor:
                formatted_results.append(f"ğŸ‘¤ Inventor: {inventor}")
            
            if filing_date:
                formatted_results.append(f"ğŸ“… Filing Date: {filing_date}")
            if priority_date:
                formatted_results.append(f"ğŸ¯ Priority Date: {priority_date}")
            if grant_date:
                formatted_results.append(f"âœ… Grant Date: {grant_date}")
            
            formatted_results.append(f"\nğŸ“ Summary:\n{snippet}")
            
            if pdf_link:
                formatted_results.append(f"\nğŸ”— PDF: {pdf_link}")
            
            formatted_results.append(f"\n")
        
        formatted_results.append("=" * 80)
        return "\n".join(formatted_results)
        
    except Exception as e:
        error_msg = f"Error during patent search: {str(e)}"
        print(f"   âš ï¸  {error_msg}")
        return f"SEARCH_ERROR: {error_msg}"




# =========================
# 4) State Definition
# =========================
# =========================
# Plan Schema (NEW)
# =========================
class Task(TypedDict):
    """ë‹¨ì¼ Task ìŠ¤í‚¤ë§ˆ"""
    task_id: str  # ì˜ˆ: "T1", "T2"
    description: str  # Task ì„¤ëª…
    agent: str  # ìˆ˜í–‰í•  agent ì´ë¦„
    depends_on: List[str]  # ì„ í–‰ task_id ë¦¬ìŠ¤íŠ¸
    parallelizable: bool  # ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥ ì—¬ë¶€
    max_retries: int  # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
    inputs: Dict[str, Any]  # ì…ë ¥ ë°ì´í„°

class Plan(TypedDict):
    """ì „ì²´ Plan ìŠ¤í‚¤ë§ˆ"""
    tasks: List[Task]
    goal: str

# =========================
# Enhanced State (ê¸°ì¡´ + Plan ê´€ë ¨ í•„ë“œ)
# =========================
class State(MessagesState):
    patent_id: str  # PDF ê²½ë¡œ ë˜ëŠ” íŠ¹í—ˆ ë²ˆí˜¸
    preprocessed: bool = False  # ì „ì²˜ë¦¬ ì™„ë£Œ ì—¬ë¶€
    plan: Optional[Plan] = None  # Plannerê°€ ìƒì„±í•œ Plan
    task_results: Dict[str, Any] = {}  # {task_id: result}
    completed_tasks: Set[str] = set()  # ì™„ë£Œëœ task_id ì§‘í•©
    failed_tasks: Dict[str, str] = {}  # {task_id: error_message}
    current_iteration: int = 0  # Supervisor ì‹¤í–‰ íšŸìˆ˜
    merged_result: str = ""  # Supervisorê°€ mergeí•œ ìµœì¢… ê²°ê³¼
    next: str = ""  # ë‹¤ìŒ ë…¸ë“œ


# =========================
# 5) Preprocess Node (ìˆ˜ì •ë¨ - ì €ì¥/ë¡œë“œ ë¡œì§ ì¶”ê°€)
# =========================

def preprocess_node(state: State) -> Command[Literal["planner"]]:
    """
    PDFë¥¼ ë¡œë“œí•˜ê³  LLMìœ¼ë¡œ ì „ì²˜ë¦¬í•œ í›„ Vector DBì— ì €ì¥í•˜ëŠ” ë…¸ë“œ
    ì´ë¯¸ ì €ì¥ëœ íŠ¹í—ˆëŠ” ë¡œë“œë§Œ ìˆ˜í–‰
    """
    global vectorstore, current_patent_id
    
    patent_path = state["patent_id"]
    
    # íŠ¹í—ˆ ID ì¶”ì¶œ
    patent_filename = os.path.basename(patent_path)
    current_patent_id = os.path.splitext(patent_filename)[0]
    
    print(f"\n{'='*80}")
    print(f"Starting preprocessing for Patent ID: {current_patent_id}")
    print(f"{'='*80}\n")
    
    # ì´ë¯¸ vectorstoreê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if vectorstore_exists(current_patent_id):
        print(f"âœ… Vectorstore already exists for {current_patent_id}")
        print(f"ğŸ“‚ Loading from disk...\n")
        
        # ê¸°ì¡´ vectorstore ë¡œë“œ
        vectorstore = load_vectorstore(current_patent_id, emb)
        
        print(f"\n{'='*80}")
        print(f"âœ… Loaded existing vectorstore for {current_patent_id}")
        print(f"{'='*80}\n")
        
        return Command(
            update={"preprocessed": True},
            goto="planner"
        )
    
    # ìƒˆë¡œìš´ ì „ì²˜ë¦¬ ì‹œì‘
    print(f"ğŸ”„ No existing vectorstore found. Starting new preprocessing...\n")
    
    # ë¡œê·¸ íŒŒì¼ ìƒì„±
    preprocessing_log = create_log_file(current_patent_id, "preprocessing")
    chunking_log = create_log_file(current_patent_id, "chunking")
    
    log_and_print(f"{'='*80}", preprocessing_log)
    log_and_print(f"ì „ì²˜ë¦¬ ì‹œì‘: {current_patent_id}", preprocessing_log)
    log_and_print(f"ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", preprocessing_log)
    log_and_print(f"{'='*80}\n", preprocessing_log)
    
    # 1) PDF ë¡œë“œ (ì²« í˜ì´ì§€ ì¹¼ëŸ¼ ë¶„ë¦¬)
    pages = load_pages_with_first_page_columns(patent_path)
    log_and_print(f"Loaded {len(pages)} pages from PDF", preprocessing_log)
    
    # 2) full_text ìƒì„±
    full_text = ""
    for page in pages:
        full_text += page.page_content + "\n\n"
    
    log_and_print(f"Generated full_text with {len(full_text)} characters", preprocessing_log)
    
    # 3) LLMìœ¼ë¡œ ì „ì²˜ë¦¬ (ì„¹ì…˜ ë¶„ë¦¬ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ)
    system_prompt = """ë‹¹ì‹ ì€ ë¯¸êµ­ íŠ¹í—ˆ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ íŠ¹í—ˆ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì„¸ìš”:

1. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ:
ë¬¸ì„œì˜ ì²« í˜ì´ì§€(Front Page)ì—ì„œ ì•„ë˜ í•­ëª©ë“¤ì„ ê°€ëŠ¥í•œ í•œ ëª¨ë‘ ì¶”ì¶œí•˜ì—¬ JSON í˜•íƒœë¡œ ì •ë¦¬í•˜ì„¸ìš”.
ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í•­ëª©ì€ ë¹ˆ ë¬¸ìì—´("")ë¡œ ì±„ìš°ì„¸ìš”.

í¬í•¨í•´ì•¼ í•˜ëŠ” ì£¼ìš” ë©”íƒ€ë°ì´í„° í•„ë“œ:
- patent_number: íŠ¹í—ˆ ë“±ë¡ë²ˆí˜¸ (ì˜ˆ: "US 8,526,476 B2")
- publication_number: ê³µê°œë²ˆí˜¸ (ê³µê°œíŠ¹í—ˆì¼ ê²½ìš°)
- application_number: ì¶œì›ë²ˆí˜¸ (ì˜ˆ: "13/113,482")
- filing_date: ì¶œì›ì¼ (ì˜ˆ: "May 23, 2011")
- publication_date: ê³µê°œì¼ ë˜ëŠ” ë“±ë¡ì¼ (ì˜ˆ: "Sep. 3, 2013")
- priority_date: ìš°ì„ ê¶Œ ì£¼ì¥ì¼
- title: ë°œëª…ì˜ ëª…ì¹­
- inventor: ë°œëª…ì ì´ë¦„ ëª©ë¡
- assignee: ì–‘ìˆ˜ì¸ ë˜ëŠ” ì¶œì›ì¸ (íšŒì‚¬ëª… ë“±)
- examiner: ì‹¬ì‚¬ê´€ ì´ë¦„
- attorney_or_agent: ëŒ€ë¦¬ì¸ ë˜ëŠ” ë²•ë¥  ì‚¬ë¬´ì†Œ
- cpc_class: Cooperative Patent Classification (CPC ì½”ë“œ)
- ipc_class: International Patent Classification (IPC ì½”ë“œ)
- us_class: U.S. Classification (USPC ì½”ë“œ)
- field_of_search: Field of Classification Search
- references_cited: ì¸ìš© ë¬¸í—Œ ë˜ëŠ” íŠ¹í—ˆ ëª©ë¡
- related_applications: ê´€ë ¨ ì¶œì› ì •ë³´ (continuation, divisional ë“±)
- government_interest: ì •ë¶€ ì§€ì› ê´€ë ¨ ë‚´ìš©

2. ì„¹ì…˜ êµ¬ë¶„:
ë³¸ë¬¸ì„ ëª…í™•í•œ ì„¹ì…˜ ì œëª©(ëª¨ë‘ ëŒ€ë¬¸ì) ê¸°ì¤€ìœ¼ë¡œ êµ¬ë¶„í•˜ì„¸ìš”.
ì„¹ì…˜ ì œëª©ì€ ì•„ë˜ ì˜ˆì‹œ ëª©ë¡ì— í¬í•¨ë˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.
ë¬¸ì„œì— ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì„¹ì…˜ì„ ëª¨ë‘ ì‹ë³„í•˜ì—¬ í¬í•¨í•˜ì„¸ìš”.
ê° ì„¹ì…˜ì˜ ë‚´ìš©ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

[ì„¹ì…˜ ë‚´ìš© ì •ì œ ê·œì¹™]
- PDF parsing ì¤‘ ì˜ëª» ì¸ì‹ëœ ì •ë³´(ì˜ˆ: "Publication No.", "Publication Date", "Page X of Y", "Line X" ë“±)ëŠ” ì„¹ì…˜ ë³¸ë¬¸ì—ì„œ ì œê±°í•˜ì„¸ìš”.
- ì¦‰, 'publication_number', 'publication_date', 'page', 'line', 'sheet' ë“±ê³¼ ê´€ë ¨ëœ ê°’ì´ ì„¹ì…˜ ë‚´ìš© ì•ˆì— ìˆìœ¼ë©´ ëª¨ë‘ ì‚­ì œí•´ì•¼ í•©ë‹ˆë‹¤.
- ë©”íƒ€ë°ì´í„°ì— í•´ë‹¹í•˜ëŠ” ì •ë³´(ì˜ˆ: íŠ¹í—ˆë²ˆí˜¸, ê³µê°œë²ˆí˜¸, ì¶œì›ë²ˆí˜¸, ê³µê°œì¼ ë“±)ëŠ” ì„¹ì…˜ ë³¸ë¬¸ì— í¬í•¨í•˜ì§€ ë§ê³ , ì˜¤ì§ metadata í•„ë“œì—ë§Œ í¬í•¨í•˜ì„¸ìš”.
- ë³¸ë¬¸ ì¤‘ OCR ë…¸ì´ì¦ˆë‚˜ í‘œ, ë²ˆí˜¸, í˜ì´ì§€ ì¸ë±ìŠ¤ ë“± ë¬¸ì„œ êµ¬ì¡°ìƒ ì˜ë¯¸ ì—†ëŠ” í…ìŠ¤íŠ¸ëŠ” ì œì™¸í•˜ì„¸ìš”.
- SECTION TITLE ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë˜, SECTION ë‚´ìš©ì€ ì˜ë¯¸ ìˆëŠ” ë³¸ë¬¸ë§Œ í¬í•¨í•˜ì„¸ìš”.

ì£¼ìš” ì„¹ì…˜ ì˜ˆì‹œ (ì°¸ê³ ìš©):
- ABSTRACT
- FIELD OF THE INVENTION / TECHNICAL FIELD
- BACKGROUND OF THE INVENTION
- SUMMARY OF THE INVENTION
- BRIEF DESCRIPTION OF THE DRAWINGS
- DETAILED DESCRIPTION OF THE INVENTION
- CLAIMS (ì´ ì„¹ì…˜ì€ ë³„ë„ ì²˜ë¦¬í•˜ì§€ ë§ê³  ì›ë¬¸ ê·¸ëŒ€ë¡œ í¬í•¨) (ì´ ì„¹ì…˜ì€ "THE INVENTION CLAIMED IS"ê°€ ì•„ë‹Œ "CLAIMS"ë¡œ í†µì¼í•˜ì„¸ìš”)

3. CLAIMS ë¶„ì„ (ì¤‘ìš”!):
CLAIMS ì„¹ì…˜ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°, ê° ê°œë³„ claimì„ ì‹ë³„í•˜ê³  ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:
- claim_no: claim ë²ˆí˜¸ (ì •ìˆ˜)
- claim_text: claim ì „ì²´ í…ìŠ¤íŠ¸ (ì›ë¬¸ ê·¸ëŒ€ë¡œ)
- independent: ë…ë¦½í•­ ì—¬ë¶€ (true/false)
* ë…ë¦½í•­: ë‹¤ë¥¸ claimì„ ì°¸ì¡°í•˜ì§€ ì•ŠëŠ” claim
* ì¢…ì†í•­: "claim X", "claims X-Y", "any of claims", "according to claim" ë“±ì˜ í‘œí˜„ìœ¼ë¡œ ë‹¤ë¥¸ claimì„ ì°¸ì¡°í•˜ëŠ” claim

ë…ë¦½í•­/ì¢…ì†í•­ íŒë‹¨ ê¸°ì¤€:
- ë‹¤ìŒ íŒ¨í„´ì´ ìˆìœ¼ë©´ ì¢…ì†í•­ìœ¼ë¡œ íŒë‹¨:
* "the [noun] of claim [number]" (ì˜ˆ: "The method of claim 1")
* "the [noun] of any of claims [number]" (ì˜ˆ: "The device of any of claims 1-5")
* "according to claim [number]"
* "as recited in claim [number]"
* "dependent on claim [number]"
* "wherein the [noun] of claim [number]"
* "further according to" / "further comprising" (ë‹¨ë…ìœ¼ë¡œëŠ” ì¢…ì†í•­ ì•„ë‹˜, í•˜ì§€ë§Œ claim ì°¸ì¡°ì™€ í•¨ê»˜ ë‚˜ì˜¤ë©´ ì¢…ì†í•­)
* "a [noun] as in claim [number]"
* "characterized in that" (ìœ ëŸ½ì‹ ì¢…ì†í•­ í‘œí˜„)
- ìœ„ íŒ¨í„´ì´ ì—†ìœ¼ë©´ ë…ë¦½í•­ìœ¼ë¡œ íŒë‹¨

ì‘ë‹µì€ ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”:

{
  "metadata": {
    "patent_number": "",
    "publication_number": "",
    "application_number": "",
    "filing_date": "",
    "publication_date": "",
    "priority_date": "",
    "title": "",
    "inventor": "",
    "assignee": "",
    "examiner": "",
    "attorney_or_agent": "",
    "cpc_class": "",
    "ipc_class": "",
    "us_class": "",
    "field_of_search": "",
    "references_cited": "",
    "related_applications": "",
    "government_interest": ""
  },
  "sections": {
    "SECTION_TITLE_1": "í•´ë‹¹ ì„¹ì…˜ì˜ ì›ë¬¸ ë‚´ìš©",
    "SECTION_TITLE_2": "í•´ë‹¹ ì„¹ì…˜ì˜ ì›ë¬¸ ë‚´ìš©",
    ...
  },
  "claims": [
    {
      "claim_no": 1,
      "claim_text": "claim ì „ì²´ í…ìŠ¤íŠ¸",
      "independent": true
    },
    {
      "claim_no": 2,
      "claim_text": "claim ì „ì²´ í…ìŠ¤íŠ¸",
      "independent": false
    },
    ...
  ]
}

ì£¼ì˜:
- ì„¹ì…˜ ì œëª©ì€ ë¬¸ì„œ ë‚´ì˜ ì‹¤ì œ ëŒ€ë¬¸ì ì œëª©ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
- ì¶”ê°€ì ì¸ ì„¹ì…˜ì´ ì¡´ì¬í•˜ë©´ JSONì˜ "sections"ì— ìƒˆë¡œìš´ í‚¤ë¡œ ì¶”ê°€í•˜ì„¸ìš”.
- CLAIMS ì„¹ì…˜ì€ "sections"ì— í¬í•¨í•˜ë˜, ë™ì‹œì— "claims" ë°°ì—´ì— ê°œë³„ claimì„ ë¶„ë¦¬í•˜ì—¬ í¬í•¨í•˜ì„¸ìš”.
- claims ë°°ì—´ì˜ ìˆœì„œëŠ” ì›ë¬¸ì˜ claim ë²ˆí˜¸ ìˆœì„œë¥¼ ìœ ì§€í•˜ì„¸ìš”.
- ê° claimì˜ ë…ë¦½í•­ ì—¬ë¶€ë¥¼ ì •í™•íˆ íŒë‹¨í•˜ì—¬ "independent" í•„ë“œì— true/falseë¡œ í‘œì‹œí•˜ì„¸ìš”.
- ì¶œë ¥ì€ ë°˜ë“œì‹œ í•˜ë‚˜ì˜ JSON ê°ì²´ë¡œë§Œ êµ¬ì„±í•˜ì„¸ìš”. JSON ì´ì™¸ì˜ ì„¤ëª…, ë¬¸ì¥, í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
"""
    
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=full_text)
    ]
    
    log_and_print("Calling LLM for preprocessing...", preprocessing_log)
    # ê¸´ íŠ¹í—ˆ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ max_tokensì„ ì¶©ë¶„íˆ í¬ê²Œ ì„¤ì •
    response = llm.invoke(messages, config={"max_tokens": 16000})
    result_text = response.content
    log_and_print(f"âœ“ LLM response received ({len(result_text)} characters)", preprocessing_log)
    
    # JSON íŒŒì‹±
    try:
        # ```json ... ``` í˜•íƒœë¡œ ê°ì‹¸ì§„ ê²½ìš° ì²˜ë¦¬
        if "```json" in result_text:
            result_text = result_text.split("```json")[1].split("```")[0]
        elif "```" in result_text:
            result_text = result_text.split("```")[1].split("```")[0]
        
        patent_data = json.loads(result_text)
        log_and_print("âœ“ Successfully parsed patent data", preprocessing_log)
    except json.JSONDecodeError as e:
        error_msg = f"âŒ ì˜¤ë¥˜: {e}"
        log_and_print(error_msg, preprocessing_log)
        
        # ì „ì²´ ì‘ë‹µì„ ë¡œê·¸ íŒŒì¼ì— ì €ì¥
        log_and_print("\n" + "="*80, preprocessing_log)
        log_and_print("ì „ì²´ LLM ì‘ë‹µ:", preprocessing_log)
        log_and_print("="*80, preprocessing_log)
        log_and_print(result_text, preprocessing_log)
        log_and_print("="*80 + "\n", preprocessing_log)
        
        # ì½˜ì†”ì—ëŠ” ìš”ì•½ ì¶œë ¥
        print(f"\nâŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        print(f"ì‘ë‹µ ê¸¸ì´: {len(result_text)} ë¬¸ì")
        print(f"ì‘ë‹µ ì‹œì‘: {result_text[:500]}")
        print(f"ì‘ë‹µ ë: {result_text[-500:]}")
        print(f"\nğŸ’¡ í•´ê²° ë°©ë²•:")
        print(f"1. config.pyì—ì„œ llm ì •ì˜ì‹œ max_tokensì„ ë” í¬ê²Œ ì„¤ì • (ì˜ˆ: 16000)")
        print(f"2. ë˜ëŠ” agent_logic.pyì˜ preprocess_nodeì—ì„œ llm.invoke() í˜¸ì¶œì‹œ")
        print(f"   config={{'max_tokens': 16000}}ì„ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬")
        print(f"\nğŸ“ ìì„¸í•œ ë‚´ìš©ì€ ë¡œê·¸ íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”: {preprocessing_log}")
        raise

    # ============================================================
    # ë””ë²„ê¹…: ì „ì²˜ë¦¬ ê²°ê³¼ ì¶œë ¥ (print + log)
    # ============================================================
    log_and_print("\n" + "="*80, preprocessing_log)
    log_and_print("ğŸ“‹ ì „ì²˜ë¦¬ ê²°ê³¼ - ì„¹ì…˜ ë° Claim ìƒì„¸ ë‚´ìš©", preprocessing_log)
    log_and_print("="*80 + "\n", preprocessing_log)
    
    sections = patent_data.get('sections', {})
    claims = patent_data.get('claims', [])
    
    log_and_print("\n" + "-"*80, preprocessing_log)
    log_and_print("ğŸ“„ ì„¹ì…˜ ë‚´ìš© (ì „ì²´)", preprocessing_log)
    log_and_print("-"*80, preprocessing_log)
    for idx, (section_name, section_content) in enumerate(sections.items(), 1):
        log_and_print(f"\n{'='*80}", preprocessing_log)
        log_and_print(f"[ì„¹ì…˜ {idx}] {section_name}", preprocessing_log)
        log_and_print(f"{'='*80}", preprocessing_log)
        log_and_print(f"ê¸¸ì´: {len(section_content)} ë¬¸ì", preprocessing_log)
        log_and_print(f"\nì „ì²´ ë‚´ìš©:", preprocessing_log)
        log_and_print(section_content, preprocessing_log)
        log_and_print(f"\n{'='*80}\n", preprocessing_log)
    
    if claims:
        log_and_print("\n" + "-"*80, preprocessing_log)
        log_and_print("ğŸ“œ Claims ë‚´ìš© (ì „ì²´)", preprocessing_log)
        log_and_print("-"*80, preprocessing_log)
        for claim in claims:
            claim_type = "ë…ë¦½í•­" if claim.get('independent', False) else "ì¢…ì†í•­"
            log_and_print(f"\n{'='*80}", preprocessing_log)
            log_and_print(f"[Claim {claim['claim_no']}] ({claim_type})", preprocessing_log)
            log_and_print(f"{'='*80}", preprocessing_log)
            log_and_print(f"ê¸¸ì´: {len(claim['claim_text'])} ë¬¸ì", preprocessing_log)
            log_and_print(f"\nì „ì²´ ë‚´ìš©:", preprocessing_log)
            log_and_print(claim['claim_text'], preprocessing_log)
            log_and_print(f"\n{'='*80}\n", preprocessing_log)
    else:
        log_and_print("\nâš ï¸ Claims ì—†ìŒ", preprocessing_log)
    
    log_and_print("\n" + "="*80, preprocessing_log)
    log_and_print("âœ… ì „ì²˜ë¦¬ ê²°ê³¼ ì¶œë ¥ ì™„ë£Œ", preprocessing_log)
    log_and_print("="*80 + "\n", preprocessing_log)

    
    # 4) Documentë¡œ ë³€í™˜ (ì²­í‚¹ í¬í•¨) - ë¡œê·¸ íŒŒì¼ ì „ë‹¬
    docs = to_langchain_document(patent_data, source=patent_path, log_file=chunking_log)
    log_and_print(f"Created {len(docs)} documents", chunking_log)
    
    # 5) ëª¨ë“  ë¬¸ì„œì˜ metadataì— patent_id ì¶”ê°€ (í•„í„°ë§ì„ ìœ„í•´)
    for doc in docs:
        doc.metadata['patent_id'] = current_patent_id
    log_and_print(f"âœ“ Added patent_id to all {len(docs)} documents", chunking_log)
    
    # 6) Claims ì •ë³´ ì¶œë ¥
    claims_count = len([d for d in docs if d.metadata.get('granularity') == 'claim'])
    log_and_print(f"Found {claims_count} claims (will be retrieved based on relevance)", chunking_log)
    
    # 7) Vector store ìƒì„±
    vectorstore = FAISS.from_documents(docs, emb)
    print("âœ“ Vector store created")
    
    # 8) Vector store ì €ì¥
    save_vectorstore(vectorstore, current_patent_id)
    
    log_and_print(f"\n{'='*80}", preprocessing_log)
    log_and_print(f"âœ… Preprocessing complete for {current_patent_id}", preprocessing_log)
    log_and_print(f"ğŸ“Š Total documents: {len(docs)}", preprocessing_log)
    log_and_print(f"ğŸ“‚ Vectorstore saved to: {get_vectorstore_path(current_patent_id)}", preprocessing_log)
    log_and_print(f"ğŸ“ Preprocessing log: {preprocessing_log}", preprocessing_log)
    log_and_print(f"ğŸ“ Chunking log: {chunking_log}", preprocessing_log)
    log_and_print(f"{'='*80}\n", preprocessing_log)
    
    return Command(
        update={"preprocessed": True},
        goto="planner"
    )


# =========================
# 6) Tools for Each Agent
# =========================
innovation_tools = [get_available_metadata, search_by_metadata, search_by_similarity]
implementation_tools = [get_available_metadata, search_by_metadata, search_by_similarity]
technical_tools = [get_available_metadata, search_by_metadata, search_by_similarity]
horizontal_tools = [
    get_available_metadata, 
    search_by_metadata, 
    search_by_similarity,
    generate_patent_search_query,
    search_similar_patents_serpapi,
    refine_patent_search_query  # NEW: ì¿¼ë¦¬ ê°œì„  tool
]


# =========================
# 7) Agents with Prompts
# =========================
innovation_agent = create_react_agent(
    llm,
    tools=innovation_tools,
    prompt=(
        "You are an expert skilled in analyzing patents. "
        "Your task is to identify and describe the key innovation points and distinctive features "
        "that differentiate this patent\n\n"
        "Do not search similar patents\n\n"
        "First, abstract is useful so search for abstract\n\n"
        "IMPORTANT: You have THREE search tools available:\n"
        "- 'get_available_metadata': Check available metadata (sections, claims) FIRST\n"
        "- 'search_by_metadata': ONLY use when you need a specific section's full content or a specific claim number (e.g., 'abstract', 'independent claims', 'get all CLAIMS', 'get claim 1', )\n"
        "- 'search_by_similarity': USE THIS for all conceptual questions like 'innovation points', 'advantages', 'problems solved', 'benefits', or any keyword searches\n\n"
        "TOOL SELECTION RULES:\n"
        "- For questions like 'innovation points', 'advantages', 'problems solved', 'benefits', 'features' â†’ ALWAYS use search_by_similarity\n"
        "- For questions asking about specific concepts/keywords (e.g., 'stepped edge', 'etching conditions') â†’ ALWAYS use search_by_similarity\n"
        "- âš ï¸ For questions asking for 'all CLAIMS' or 'claim number X' or 'ABSTRACT' â†’ use search_by_metadata with filters\n"
#         "- When in doubt, use search_by_similarity - it works for most questions\n\n"
        "Examples:\n"
        "- 'innovation points' â†’ search_by_similarity('innovation points of this patent')\n"
        "- 'advantages of stepped edge' â†’ search_by_similarity('advantages of stepped edge')\n"
        "- 'get all dependent claims' â†’ search_by_metadata('', filters=\"metadata['section'] == 'CLAIMS' and metadata['independent'] == False\")\n"
    ),
)

implementation_agent = create_react_agent(
    llm,
    tools=implementation_tools,
    prompt=(
        "You are an expert in the fields of semiconductors, "
        "and you are very skilled at interpreting specific implementation methods in patents. "
        "Your task is to summarize and describe the implementation methods in patents.\n\n"
        "IMPORTANT: You have THREE search tools available:\n"
        "- 'get_available_metadata': Check available metadata (sections, claims) FIRST\n"
        "- 'search_by_metadata': ONLY use when you need a specific section's full content or a specific claim number\n"
        "- 'search_by_similarity': USE THIS for all conceptual searches about methods, processes, fabrication, conditions, and keywords\n\n"
        "TOOL SELECTION RULES:\n"
        "- For questions about 'implementation methods', 'processes', 'fabrication steps', 'conditions' â†’ ALWAYS use search_by_similarity\n"
        "- For questions about specific keywords (e.g., 'HCl etching', 'TMAH developer', 'temperature ranges') â†’ ALWAYS use search_by_similarity\n"
        "- For questions asking for 'all of DETAILED DESCRIPTION section' â†’ use search_by_metadata with filters\n"
        "- When searching for concepts/keywords within a section, use search_by_similarity (NOT search_by_metadata)\n\n"
        "Examples:\n"
        "- 'etching conditions' â†’ search_by_similarity('HCl etching 840-860 C TMAH developer')\n"
        "- 'fabrication method' â†’ search_by_similarity('fabrication method implementation process')\n"
        "- 'get all DETAILED DESCRIPTION' â†’ search_by_metadata('', filters=\"metadata['section'] == 'DETAILED DESCRIPTION'\")\n"
    ),
)

technical_agent = create_react_agent(
    llm,
    tools=technical_tools,
    prompt=(
        "You are an expert in the fields of semiconductors, "
        "and you are very skilled at interpreting technical details and principles in patents. "
        "Your task is to summarize and describe the technical details and principles in patents.\n\n"
        "IMPORTANT: You have THREE search tools available:\n"
        "- 'get_available_metadata': Check available metadata (sections, claims) FIRST\n"
        "- 'search_by_metadata': ONLY use when you need a specific section's full content or a specific claim number\n"
        "- 'search_by_similarity': USE THIS for all conceptual searches about technical specs, principles, materials, conditions, and keywords\n\n"
        "TOOL SELECTION RULES:\n"
        "- For questions about 'technical details', 'principles', 'mechanisms', 'specifications', 'materials' â†’ ALWAYS use search_by_similarity\n"
        "- For questions about specific technical keywords or conditions â†’ ALWAYS use search_by_similarity\n"
        "- For questions asking for 'all of a specific section' â†’ use search_by_metadata with filters\n"
        "- When searching for technical concepts/keywords, use search_by_similarity (NOT search_by_metadata)\n\n"
        "Examples:\n"
        "- 'technical details of etching' â†’ search_by_similarity('etching technical details conditions')\n"
        "- 'material properties' â†’ search_by_similarity('material properties specifications')\n"
        "- 'get all DETAILED DESCRIPTION' â†’ search_by_metadata('', filters=\"metadata['section'] == 'DETAILED DESCRIPTION'\")\n"
    ),
)


horizontal_agent = create_react_agent(
    llm,
    tools=horizontal_tools,
    prompt=(
        "You are an expert in horizontal comparison and analysis. "
        "Your task is to compare the CURRENT patent with similar patents (default: 2 patents, or N patents if user specifies) "
        "and provide a structured comparison report.\n\n"
        
        "IMPORTANT: You have SIX search tools available:\n"
        "- 'get_available_metadata': Check available metadata (sections, claims) FIRST\n"
        "- 'search_by_metadata': ONLY use when you need a specific section's full content or a specific claim number\n"
        "- 'search_by_similarity': USE THIS for all conceptual searches about key features, claims content, and keywords\n"
        "- 'generate_patent_search_query': Generate optimized search query from abstract for finding similar patents\n"
        "- 'search_similar_patents_serpapi': Search Google Patents via SerpAPI to find similar patents\n\n"
        "- 'refine_patent_search_query': Progressively refines and improves search queries:\n"
        "  * Uses GENERAL GUIDELINES to enhance query sophistication\n"
        "  * Provides constructive feedback on improvements made\n"
        "  * Returns JSON with refined_query and feedback fields\n"
        
        
        "HOW TO EXTRACT NUMBER OF PATENTS FROM USER QUERY:\n"
        "- Check if user specifies a number: \"3ê°œ\", \"5 patents\", \"find 4 similar\", etc.\n"
        "- Extract the number N from patterns like: Nê°œ, N patents, N similar, find N, search N\n"
        "- If no number specified, use default N=2\n"
        "- Use this N value in search_similar_patents_serpapi(query, num_results=N)\n"
        
        "PATENT COMPARISON WORKFLOW:\n"
        "When asked to compare patents or find similar patents, follow this EXACT workflow:\n"
        "1. Extract the current patent's ABSTRACT: search_by_metadata('', filters=\"metadata['section'] == 'ABSTRACT'\")\n"
        "2. Generate initial search query: generate_patent_search_query(abstract_text)\n"
        "3. âš ï¸âš ï¸ CRITICAL: Refine query TWO times to improve sophistication:\n"
        "   - First refinement: refine_patent_search_query(abstract, initial_query, iteration=1)\n"
        "   - Extract refined_query from JSON response\n"
        "   - Second refinement: refine_patent_search_query(abstract, first_refined_query, iteration=2)\n"
        "   - Extract refined_query from JSON response\n"
        "   - Progressive refinement ensures high-quality search query\n"
        "4. Search for similar patents: search_similar_patents_serpapi(final_refined_query, num_results=N) where N=2 by default, or N=user_specified_number if mentioned in query\n"
        "   - âš ï¸If NO_RESULTS_FOUNDâš ï¸: refine one more time and search again\n"
        "     * refine_patent_search_query(abstract, final_refined_query, iteration=3)\n"
        "     * search_similar_patents_serpapi(new_refined_query, num_results=N)\n"
        "5. Extract key features from current patent: search_by_similarity('key features innovation points claims')\n"
        "6. Create structured comparison report (see OUTPUT FORMAT below)\n\n"
        
        "OUTPUT FORMAT (MUST FOLLOW THIS STRUCTURE):\n"
        "===== PATENT COMPARISON REPORT =====\n\n"
        "## CURRENT PATENT\n"
        "- Patent ID: [ID]\n"
        "- Abstract: [Full abstract from current patent]\n"
        "- Key Innovation Points: [3-5 bullet points]\n\n"
        
        "## SIMILAR PATENT #1\n"
        "- Patent ID: [ID from search results]\n"
        "- Title: [Title from search results]\n"
        "- Assignee: [Assignee]\n"
        "- Abstract/Summary: [Snippet/summary from search results]\n\n"
        
        "## SIMILAR PATENT #2\n"
        "- Patent ID: [ID from search results]\n"
        "- Title: [Title from search results]\n"
        "- Assignee: [Assignee]\n"
        "- Abstract/Summary: [Snippet/summary from search results]\n\n"
        
        "## COMPARATIVE ANALYSIS (Current Patent vs Similar Patents)\n"
        "### 1. Technical Approach\n"
        "- Current Patent: [approach]\n"
        "- Similar Patent #1: [approach]\n"
        "- Similar Patent #2: [approach]\n\n"
        
        "### 2. Key Differences\n"
        "- What makes the current patent unique: [differences]\n\n"
        
        "### 3. Common Elements\n"
        "- Shared technical concepts: [commonalities]\n\n"
        
        "### 4. Advantages of Current Patent\n"
        "- [List advantages over similar patents]\n\n"
        
        "### 5. Potential Disadvantages\n"
        "- [List any limitations]\n\n"
        
        "CRITICAL RULES:\n"
        "- Search for similar patents (num_results=2 by default, or N if user specifies 'Nê°œ', 'N patents', etc.)\n"
        "- Always include abstract/summary for BOTH similar patents\n"
        "- Always center the comparison around the CURRENT patent\n"
        "- Use the OUTPUT FORMAT structure above\n"
        "- Be concise but comprehensive\n\n"
        
        "TOOL SELECTION RULES:\n"
        "- For 'key features', 'technical advantages' â†’ search_by_similarity\n"
        "- For specific claim content â†’ search_by_metadata with filters\n"
        "- For 'all independent/dependent claims' â†’ search_by_metadata with filters\n"
        "- For patent comparison â†’ Follow PATENT COMPARISON WORKFLOW\n"
    ),
)


# =========================
# 8) Task Templates
# =========================
innovation_req = """# Requirement:
- You need to read the patent carefully and give the abstract, innovation, strengths and weaknesses, and application prospects. Answer as much as possible from the relevant direction of the userâ€™s question.
- All your outputs must be truthful and rigorous, rejecting fabrications
- Provide detailed descriptions with quantitative figures from the patent
- The final outputs should be rendered in English

# Task Description:
Analyze the patent (ID: {patent_id}) from multiple perspectives, especially the innovative points.
"""

implementation_req = """# Requirement:
- You need to carefully read the patent content and provide specific implementation methods for the patent.
- Please note that you need to describe the implementation process of the patent in as much detail as possible. You are willing to describe it very clearly and output more text.
- Please note that you need to keep the reference to the image number in the original text during the answering process, for example, you need to add "as shown as Figâ€¦" to each of your answers.
- You are very rigorous and serious, never falsifying information. You can provide specific and accurate numbers to enrich the content. You are willing to output any details related to the patentâ€™s process.
- You only need to provide the implementation method, without outputting any other information like abstract or conclusion.
- The final outputs should be rendered in English

# Task Description:
Only tell me the implementation methods of this patent I will give. You should primarily answer based on the patent content, while also using your own knowledge as a supplement. 
Provide detailed implementation methods of the patent (ID: {patent_id})
"""

technical_req = """# Requirement:
- First, you need to carefully read the patent content.
- Then you need to add some technical details and principles based on the content of the patent. For example, what are the special design ideas, what are the preparation methods of materials, what special environmental conditions are required, and what special devices or technologies are needed, etc.
- Search for: technical details, principles, design specifications, materials, conditions
- You are very rigorous and serious, never falsifying information. You are good at discovering any details of patents. You are willing to describe it very clearly and output more text.
- You can provide specific and accurate numbers to enrich technical details. You are willing to output any details related to the patentâ€™s process.
- You only need to provide the technical details, without outputting any other information like abstract or conclusion.
- The final outputs should be rendered in English

# Task Description:
Only tell me the technical details and principles of this patent I will give. You should primarily answer based on the patent content, while also using your own knowledge as a supplement. 
Answer as detailed as possible, pay attention to providing some real numbers to increase reliability. 
Answer in English. The patent is: {patent_id}
"""

horizontal_req = """# Requirement:
- Follow the PATENT COMPARISON WORKFLOW to create a structured comparison report:
  1. Extract the current patent's ABSTRACT using search_by_metadata
  2. Generate an optimized search query using generate_patent_search_query
  3. âš ï¸ CRITICAL: Search for similar patents using search_similar_patents_serpapi(query, num_results=N) where N=2 by default, or N=user_specified_number
  4. Extract key features from the current patent using search_by_similarity
  5. Create a structured comparison report following the OUTPUT FORMAT in your instructions

- MUST include in your report:
  1. Current Patent Section:
     * Patent ID and full abstract
     * 3-5 key innovation points
  
  2. Similar Patent #1 Section:
     * Patent ID, title, assignee
     * Abstract or summary from search results
  
  3. Similar Patent #2 Section:
     * Patent ID, title, assignee
     * Abstract or summary from search results
  
  4. Comparative Analysis Section (Current Patent-centered):
     * Technical approach comparison
     * Key differences (what makes current patent unique)
     * Common elements
     * Advantages of current patent
     * Potential disadvantages

- Comparison MUST be centered around the CURRENT patent
- All outputs must be truthful, rigorous, and based on actual patent content
- Use clear sections and bullet points for readability
- The final outputs should be rendered in English

# Task Description:
Compare the patent (ID: {patent_id}) with similar patents found via Google Patents (default: 2 patents, or N if specified by user).
Provide a structured, current-patent-centered comparison report following the OUTPUT FORMAT.
Focus on what makes the current patent unique and innovative compared to the similar patents.
"""

# =========================
# 8.5) Planner Agent (Intent Detection + Plan ìƒì„±)
# =========================

# Intent detection patterns (ê¸°ì¡´ supervisorì—ì„œ ì´ë™)
KOR_INTENT = {
    "innovation": [
        r"í˜ì‹ \s*í¬ì¸íŠ¸", r"í˜ì‹ \s*ì ", r"ì°¨ë³„í™”", r"novel", r"innovation", r"í•µì‹¬", r"íŠ¹ì§•"
    ],
    "implementation": [
        r"êµ¬í˜„", r"ê³µì •", r"ì ˆì°¨", r"ë°©ë²•", r"implementation", r"process", r"ì œì¡°"
    ],
    "technical": [
        r"ê¸°ìˆ \s*ì„¸ë¶€", r"ì›ë¦¬", r"ë©”ì»¤ë‹ˆì¦˜", r"technical", r"principle", r"ìƒì„¸"
    ],
    "horizontal": [
        r"ë¹„êµ", r"ìˆ˜í‰", r"ìœ ì‚¬\s*íŠ¹í—ˆ", r"similar", r"compare", r"ëŒ€ì¡°", r"ê²€ìƒ‰"
    ],
}

def detect_intents(text: str) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ ì—¬ëŸ¬ intentë¥¼ ê°ì§€"""
    detected = []
    
    checks = [
        ("innovation_agent", KOR_INTENT["innovation"]),
        ("implementation_agent", KOR_INTENT["implementation"]),
        ("technical_agent", KOR_INTENT["technical"]),
        ("horizontal_agent", KOR_INTENT["horizontal"]),
    ]
    
    for agent, patterns in checks:
        for p in patterns:
            if re.search(p, text, re.IGNORECASE):
                if agent not in detected:
                    detected.append(agent)
                break
    
    return detected

PLANNER_SYSTEM_PROMPT = """You are an expert task planner for patent analysis workflows.

Your role:
1. Analyze the user's query to understand what they want to know about the patent
2. Decompose complex queries into minimal independent sub-tasks
3. Create a DAG (Directed Acyclic Graph) of tasks with proper dependencies
4. Assign each task to the appropriate expert agent

Available Agents:
- innovation_agent: Identifies innovation points, key features, advantages, novelty
- implementation_agent: Explains implementation methods, processes, fabrication steps, procedures
- technical_agent: Describes technical details, principles, specifications, mechanisms
- horizontal_agent: Compares with other similar patents using Google Patents search, identifies unique aspects and differences

Task Planning Guidelines:
1. **Intent Detection**: Carefully analyze what the user is asking for
   - Keywords like "í˜ì‹ ", "innovation", "í•µì‹¬", "íŠ¹ì§•" â†’ innovation_agent
   - Keywords like "êµ¬í˜„", "ë°©ë²•", "ê³µì •", "ì œì¡°" â†’ implementation_agent  
   - Keywords like "ê¸°ìˆ ", "ì›ë¦¬", "ë©”ì»¤ë‹ˆì¦˜", "ìƒì„¸" â†’ technical_agent
   - Keywords like "ë¹„êµ", "compare", "ìœ ì‚¬", "ë‹¤ë¥¸ íŠ¹í—ˆ", "similar patents" â†’ horizontal_agent

2. **Single Agent for Simple Queries**: If the query asks for only ONE thing, create only ONE task
   - "ì´ íŠ¹í—ˆì˜ í˜ì‹  í¬ì¸íŠ¸ë¥¼ ì•Œë ¤ì¤˜" â†’ Only innovation_agent (1 task)
   - "êµ¬í˜„ ë°©ë²•ì„ ì„¤ëª…í•´ì¤˜" â†’ Only implementation_agent (1 task)

3. **Multiple Agents for Complex Queries**: If query asks for MULTIPLE things, create multiple tasks
   - "í˜ì‹  í¬ì¸íŠ¸ì™€ êµ¬í˜„ ë°©ë²•ì„ ì•Œë ¤ì¤˜" â†’ innovation_agent + implementation_agent (2 tasks)
   - Task order matters: innovation usually comes before implementation

4. **Dependencies**: 
   - If one task needs results from another, use depends_on
   - If tasks are independent, mark parallelizable=true

5. **Clear Inputs**:
   - First task: Use {"query": "specific question for this agent"}
   - Dependent tasks: Use {"use_result_from": "T1"} or {"query": "..."}

Output Format (JSON only):
{{
  "goal": "Clear description of what user wants to know",
  "tasks": [
    {{
      "task_id": "T1",
      "description": "Brief task description",
      "agent": "innovation_agent",
      "depends_on": [],
      "parallelizable": true,
      "max_retries": 2,
      "inputs": {{"query": "Specific question"}}
    }}
  ]
}}

CRITICAL RULES:
- Output ONLY valid JSON, no other text
- For simple queries, create ONLY ONE task
- Match agent to user's actual question
- Task IDs: T1, T2, T3... (sequential)
- Validate dependencies (no cycles)
"""


class PlanOutput(TypedDict):
    """Plannerì˜ êµ¬ì¡°í™”ëœ ì¶œë ¥"""
    goal: str
    tasks: List[Task]

def planner_node(state: State) -> Command[Literal["supervisor"]]:
    """
    Planner Agent: Query ë¶„ì„ + Plan ìƒì„±
    ì¢…í•© ë¶„ì„ ìš”ì²­ ì‹œ ëª¨ë“  agentë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
    """
    print("\n" + "="*80)
    print("ğŸ§  PLANNER - Plan Creation")
    print("="*80)
    
    # ì‚¬ìš©ì ì¿¼ë¦¬ ì¶”ì¶œ
    user_query = ""
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            user_query = msg.content
            break
    
    print(f"\nğŸ“ User Query: {user_query}")
    print(f"ğŸ“„ Patent ID: {state['patent_id']}")
    
    # ============================================
    # ğŸ†• ì¢…í•© ë¶„ì„ íŒ¨í„´ ê°ì§€
    # ============================================
    comprehensive_patterns = [
        r"ì¢…í•©\s*ë¶„ì„",
        r"ì „ì²´\s*ë¶„ì„", 
        r"ëª¨ë“ \s*ê²ƒ",
        r"ëª¨ë‘\s*ë¶„ì„",
        r"ì™„ì „í•œ\s*ë¶„ì„",
        r"comprehensive\s+analys",
        r"complete\s+analys",
        r"full\s+analys",
    ]
    
    # ì¼ë°˜ì ì¸ "ë¶„ì„í•´ì¤˜" íŒ¨í„´ (ë‹¤ë¥¸ í‚¤ì›Œë“œê°€ ì—†ì„ ë•Œë§Œ)
    simple_analysis_patterns = [
        r"^ë¶„ì„í•´ì¤˜?\.?$",
        r"^ë¶„ì„\s*í•´\s*ì¤˜?\.?$",
        r"^analyz",
        r"^tell\s+me\s+about",
        r"^explain\s+this\s+patent"
    ]
    
    is_comprehensive = False
    
    # ì¢…í•© ë¶„ì„ íŒ¨í„´ ì²´í¬
    for pattern in comprehensive_patterns:
        if re.search(pattern, user_query, re.IGNORECASE):
            is_comprehensive = True
            print("\nğŸ¯ Comprehensive analysis request detected!")
            break
    
    # ì¼ë°˜ ë¶„ì„ íŒ¨í„´ ì²´í¬ (íŠ¹ì • í‚¤ì›Œë“œê°€ ì—†ì„ ë•Œë§Œ)
    if not is_comprehensive:
        for pattern in simple_analysis_patterns:
            if re.search(pattern, user_query, re.IGNORECASE):
                # ë‹¤ë¥¸ íŠ¹ì • í‚¤ì›Œë“œê°€ ì—†ëŠ”ì§€ í™•ì¸
                specific_keywords = ["í˜ì‹ ", "êµ¬í˜„", "ê¸°ìˆ ", "ë¹„êµ", "innovation", "implementation", "technical", "compare", "horizontal"]
                has_specific = any(kw in user_query.lower() for kw in specific_keywords)
                
                if not has_specific:
                    is_comprehensive = True
                    print("\nğŸ¯ General analysis request detected (no specific agent mentioned)!")
                    break
    
    # ============================================
    # ì¢…í•© ë¶„ì„ Plan ìë™ ìƒì„±
    # ============================================
    if is_comprehensive:
        print("\nğŸ“‹ Creating comprehensive analysis plan with all agents...")
        
        comprehensive_plan = Plan(
            goal="Comprehensive analysis of the patent covering all aspects: innovation points, implementation methods, technical details, and comparison with similar patents",
            tasks=[
                Task(
                    task_id="T1",
                    description="Analyze innovation points and key features",
                    agent="innovation_agent",
                    depends_on=[],
                    parallelizable=False,
                    max_retries=2,
                    inputs={"query": "Analyze the innovation points, key features, advantages, and distinctive aspects of this patent in detail."}
                ),
                Task(
                    task_id="T2",
                    description="Explain implementation methods and processes",
                    agent="implementation_agent",
                    depends_on=["T1"],
                    parallelizable=False,
                    max_retries=2,
                    inputs={"query": "Describe the implementation methods, fabrication processes, and procedural steps of this patent in detail. Include figure references where applicable."}
                ),
                Task(
                    task_id="T3",
                    description="Describe technical details and principles",
                    agent="technical_agent",
                    depends_on=["T2"],
                    parallelizable=False,
                    max_retries=2,
                    inputs={"query": "Explain the technical details, principles, mechanisms, material specifications, and design considerations of this patent in detail."}
                ),
                Task(
                    task_id="T4",
                    description="Compare with similar patents",
                    agent="horizontal_agent",
                    depends_on=["T3"],
                    parallelizable=False,
                    max_retries=2,
                    inputs={"query": "Find similar patents (2 patents) and create a comprehensive comparison report highlighting what makes the current patent unique and innovative."}
                )
            ]
        )
        
        print("\nâœ… Comprehensive Plan Created!")
        print("\n" + "="*80)
        print("ğŸ“‹ COMPREHENSIVE EXECUTION PLAN")
        print("="*80)
        print(f"ğŸ¯ Goal: {comprehensive_plan['goal']}")
        print(f"ğŸ“Š Total Tasks: {len(comprehensive_plan['tasks'])}")
        print("\n" + "-"*80)
        
        for i, task in enumerate(comprehensive_plan['tasks'], 1):
            deps = ", ".join(task['depends_on']) if task['depends_on'] else "None"
            
            print(f"\n[Task {i}] {task['task_id']}: {task['description']}")
            print(f"  Agent: {task['agent']}")
            print(f"  Dependencies: {deps}")
            print(f"  Input: {task['inputs']['query'][:80]}...")
            print("  " + "-"*40)
        
        print("\n" + "="*80)
        print("ğŸš€ Starting comprehensive analysis...")
        print("="*80)
        
        return Command(
            update={
                "plan": comprehensive_plan,
                "task_results": {},
                "completed_tasks": set(),
                "failed_tasks": {},
                "current_iteration": 0
            },
            goto="supervisor"
        )
    
    # ============================================
    # ì¼ë°˜ Plan ìƒì„± (ê¸°ì¡´ ë¡œì§)
    # ============================================
    
    # Intent detection
    detected_intents = detect_intents(user_query)
    if detected_intents:
        print(f"\nğŸ” Detected Intents: {', '.join(detected_intents)}")
    else:
        print("\nğŸ” No specific intents detected, will use LLM judgment")
    
    # Plan ìƒì„±
    planner_messages = [
        SystemMessage(content=PLANNER_SYSTEM_PROMPT),
        HumanMessage(content=f"""
User Query: {user_query}
Patent ID: {state['patent_id']}

Detected intents: {detected_intents if detected_intents else 'None - use your judgment'}

Analyze the query carefully and create an appropriate execution plan.
For simple queries asking for ONE thing, create only ONE task.
Output ONLY the JSON plan.
""")
    ]
    
    # LLM í˜¸ì¶œ
    try:
        response = llm.with_structured_output(PlanOutput).invoke(planner_messages)
        plan = Plan(goal=response["goal"], tasks=response["tasks"])
        
        print("\nâœ… Plan Created Successfully!")
        print("\n" + "="*80)
        print("ğŸ“‹ EXECUTION PLAN")
        print("="*80)
        print(f"ğŸ¯ Goal: {plan['goal']}")
        print(f"ğŸ“Š Total Tasks: {len(plan['tasks'])}")
        print("\n" + "-"*80)
        
        for i, task in enumerate(plan['tasks'], 1):
            deps = ", ".join(task['depends_on']) if task['depends_on'] else "None"
            parallel = "âœ“ Yes" if task['parallelizable'] else "âœ— No"
            
            print(f"\n[Task {i}] {task['task_id']}: {task['description']}")
            print(f"  Agent: {task['agent']}")
            print(f"  Dependencies: {deps}")
            print(f"  Parallelizable: {parallel}")
            
            # Inputs ì¶œë ¥
            if 'use_result_from' in task['inputs']:
                print(f"  Input: Uses result from {task['inputs']['use_result_from']}")
            elif 'query' in task['inputs']:
                query_preview = task['inputs']['query'][:60] + "..." if len(task['inputs']['query']) > 60 else task['inputs']['query']
                print(f"  Input: {query_preview}")
            
            print("  " + "-"*40)
        
        print("\n" + "="*80)
        print("ğŸš€ Starting execution...")
        print("="*80)
        
        return Command(
            update={
                "plan": plan,
                "task_results": {},
                "completed_tasks": set(),
                "failed_tasks": {},
                "current_iteration": 0
            },
            goto="supervisor"
        )
        
    except Exception as e:
        print(f"\nâŒ Error creating plan: {e}")
        
        # Fallback: ê°€ì¥ ê¸°ë³¸ì ì¸ í”Œëœ
        # Intent detection ê²°ê³¼ í™œìš©
        if detected_intents:
            agent = detected_intents[0]  # ì²« ë²ˆì§¸ intent ì‚¬ìš©
        else:
            agent = "innovation_agent"  # ê¸°ë³¸ê°’
        
        fallback_plan = Plan(
            goal=user_query,
            tasks=[
                Task(
                    task_id="T1",
                    description=user_query,
                    agent=agent,
                    depends_on=[],
                    parallelizable=True,
                    max_retries=2,
                    inputs={"query": user_query}
                )
            ]
        )
        
        print(f"\nâš ï¸ Using fallback plan: Single task with {agent}")
        
        return Command(
            update={
                "plan": fallback_plan,
                "task_results": {},
                "completed_tasks": set(),
                "failed_tasks": {},
                "current_iteration": 0
            },
            goto="supervisor"
        )

# =========================
# 9) Node Runner (ê¸°ì¡´ + Task ê¸°ë°˜ í™•ì¥)
# =========================
def _run_agent(agent, task_template: str, state: State, name: str) -> Command[Literal["supervisor"]]:
    """ê¸°ì¡´ ë°©ì‹ì˜ agent ì‹¤í–‰ (legacy, í˜¸í™˜ì„± ìœ ì§€)"""
    task = task_template.format(patent_id=state["patent_id"])
    user_msg = HumanMessage(content=task, name=name)
    
    result = agent.invoke({"messages": state["messages"] + [user_msg]})
    
    completed = state.get("completed_agents", [])
    return Command(
        update={
            "messages": result["messages"],
            "completed_agents": completed + [name] if name not in completed else completed
        },
        goto="supervisor"
    )

def _run_task(agent, task: Task, task_template: str, state: State, name: str) -> Command[Literal["supervisor"]]:
    """NEW: Task ê¸°ë°˜ agent ì‹¤í–‰ (í…œí”Œë¦¿ ì‚¬ìš©) - ì´ì „ ê²°ê³¼ë¥¼ ì°¸ê³ """
    task_id = task['task_id']
    task_results = state.get("task_results", {})
    previous_context = state.get("previous_context", "")
    
    print(f"\n{'='*80}")
    print(f"ğŸ¤– [{name}] Executing Task: {task_id}")
    print(f"{'='*80}")
    print(f"ğŸ“ Description: {task['description']}")
    
    # 1. ê¸°ì¡´ í…œí”Œë¦¿ì˜ ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ì ìš©
    base_requirements = task_template.format(patent_id=state["patent_id"])
    
    # 2. Taskì˜ êµ¬ì²´ì ì¸ ì…ë ¥ êµ¬ì„±
    task_inputs = task['inputs']
    
    if 'use_result_from' in task_inputs:
        # ì´ì „ task ê²°ê³¼ ì‚¬ìš©
        ref_task_id = task_inputs['use_result_from']
        if ref_task_id in task_results:
            context = task_results[ref_task_id]
            specific_query = f"{task['description']}\n\nContext from previous task [{ref_task_id}]:\n{context}"
            print(f"\nğŸ“ Using result from: {ref_task_id}")
        else:
            specific_query = task['description']
            print(f"\nâš ï¸ Warning: Referenced task {ref_task_id} result not found")
    elif 'query' in task_inputs:
        specific_query = task_inputs['query']
    else:
        specific_query = task['description']
    
    # 3. ì´ì „ taskë“¤ì˜ context ì¶”ê°€ (ìˆìœ¼ë©´)
    if previous_context:
        specific_query = f"{specific_query}{previous_context}"
        print(f"\nğŸ“š Including context from previous tasks")
    
    # 4. ìµœì¢… ì¿¼ë¦¬ = ê¸°ë³¸ í…œí”Œë¦¿ + êµ¬ì²´ì ì¸ task ë‚´ìš© + ì´ì „ context
    final_query = f"{base_requirements}\n\n# Specific Task for this execution:\n{specific_query}"
    
    print(f"\nğŸ’¬ Using template: {name}")
    print(f"ğŸ“‹ Task-specific query: {specific_query[:100]}..." if len(specific_query) > 100 else f"ğŸ“‹ Task-specific query: {specific_query}")
    
    # Agent ì‹¤í–‰
    try:
        user_msg = HumanMessage(content=final_query, name=name)
        
        result = agent.invoke({"messages": state["messages"] + [user_msg]})
        
        # ê²°ê³¼ ì¶”ì¶œ
        if result and "messages" in result and result["messages"]:
            last_msg = result["messages"][-1]
            output = last_msg.content if hasattr(last_msg, 'content') else str(last_msg)
        else:
            output = "No response from agent"
        
        print(f"\nâœ… Task {task_id} completed successfully")
        print(f"ğŸ“Š Output length: {len(output)} characters")
        
        # Update completed tasks
        new_completed = state.get("completed_tasks", set()) | {task_id}
        print(f"âœ“ Marking task as completed: {task_id}")
        print(f"âœ“ Total completed tasks: {len(new_completed)} - {sorted(new_completed)}")
        print(f"{'='*80}\n")
        
        return Command(
            update={
                "task_results": {**task_results, task_id: output},
                "completed_tasks": new_completed,
                "messages": result["messages"]
            },
            goto="supervisor"
        )
        
    except Exception as e:
        error_msg = f"Error in {name}: {str(e)}"
        print(f"\nâŒ {error_msg}")
        print(f"{'='*80}\n")
        
        failed = state.get("failed_tasks", {})
        failed[task_id] = error_msg
        
        return Command(
            update={"failed_tasks": failed},
            goto="supervisor"
        )


# =========================
# 10) Agent Nodes
# =========================
def innovation_node(state: State) -> Command[Literal["supervisor"]]:
    # Task ê¸°ë°˜ ì‹¤í–‰ì¸ì§€ í™•ì¸
    if "task" in state:
        return _run_task(innovation_agent, state["task"], innovation_req, state, "innovation_agent")
    # Legacy ë°©ì‹
    return _run_agent(innovation_agent, innovation_req, state, "innovation_agent")

def implementation_node(state: State) -> Command[Literal["supervisor"]]:
    # Task ê¸°ë°˜ ì‹¤í–‰ì¸ì§€ í™•ì¸
    if "task" in state:
        return _run_task(implementation_agent, state["task"], implementation_req, state, "implementation_agent")
    # Legacy ë°©ì‹
    return _run_agent(implementation_agent, implementation_req, state, "implementation_agent")

def technical_node(state: State) -> Command[Literal["supervisor"]]:
    # Task ê¸°ë°˜ ì‹¤í–‰ì¸ì§€ í™•ì¸
    if "task" in state:
        return _run_task(technical_agent, state["task"], technical_req, state, "technical_agent")
    # Legacy ë°©ì‹
    return _run_agent(technical_agent, technical_req, state, "technical_agent")

def horizontal_node(state: State) -> Command[Literal["supervisor"]]:
    # Task ê¸°ë°˜ ì‹¤í–‰ì¸ì§€ í™•ì¸
    if "task" in state:
        return _run_task(horizontal_agent, state["task"], horizontal_req, state, "horizontal_agent")
    # Legacy ë°©ì‹
    return _run_agent(horizontal_agent, horizontal_req, state, "horizontal_agent")


# =========================
# 11) Supervisor (Plan ì „ìš© - Merge + ê²°ê³¼ ìš”ì•½)
# =========================

MAX_SUPERVISOR_ITERATIONS = 10  # ë¬´í•œë£¨í”„ ë°©ì§€

def get_ready_tasks(plan: Plan, completed: Set[str], failed: Set[str]) -> List[Task]:
    """ì‹¤í–‰ ê°€ëŠ¥í•œ taskë“¤ì„ ë°˜í™˜ (ì˜ì¡´ì„±ì´ ëª¨ë‘ ì™„ë£Œëœ task)"""
    ready = []
    
    for task in plan['tasks']:
        task_id = task['task_id']
        
        # ì´ë¯¸ ì™„ë£Œë˜ì—ˆê±°ë‚˜ ì‹¤íŒ¨í•œ taskëŠ” ì œì™¸
        if task_id in completed or task_id in failed:
            continue
        
        # ì˜ì¡´ì„± í™•ì¸
        deps = task['depends_on']
        
        # ì˜ì¡´ì„±ì´ ì—†ê±°ë‚˜, ëª¨ë“  ì˜ì¡´ì„±ì´ ì™„ë£Œëœ ê²½ìš°
        if not deps or all(dep in completed for dep in deps):
            ready.append(task)
    
    return ready

def merge_task_results(plan: Plan, task_results: Dict[str, Any], completed: Set[str]) -> str:
    """ì™„ë£Œëœ taskë“¤ì˜ ê²°ê³¼ë¥¼ í†µí•©"""
    
    print("\n" + "-"*80)
    print("ğŸ“ Merging task results...")
    print("-"*80)
    
    merged = f"# Analysis of Patent\n\n"
    merged += f"**Goal**: {plan['goal']}\n\n"
    merged += "---\n\n"
    
    # ê²°ê³¼ ì¶”ê°€
    if task_results:
        print(f"  Including {len(completed)} results from execution")
        for task_id in sorted(completed):
            task = next((t for t in plan['tasks'] if t['task_id'] == task_id), None)
            if task and task_id in task_results:
                merged += f"### {task['description']}\n"
                merged += f"*(Analyzed by {task['agent']})*\n\n"
                merged += task_results[task_id]
                merged += "\n\n"
    
    print(f"âœ“ Total sections merged: {len(completed)}")
    print(f"âœ“ Total length: {len(merged)} characters")
    
    return merged

FINAL_SUMMARIZATION_PROMPT = """You are finalizing the analysis results for the user.

You have completed multiple tasks analyzing a patent, and the results have been merged together.
However, the merged result may contain:
- Redundant information from multiple executions
- Poor structure or organization
- Difficult-to-read formatting

Your task:
Transform the merged results into a clear, well-structured, easy-to-read final report.

Guidelines:
1. **Use Plan Structure**: Organize the report following the execution plan's task descriptions
   - Use numbered sections (1., 2., 3., etc.) for each task
   - Use the task description as the section title
   - Clearly indicate which agent analyzed each section

2. **Remove Redundancy**: If the same information appears multiple times, keep only the best version

3. **Enhance Readability**: 
   - Use proper markdown formatting
   - Add clear headers and subheaders
   - Use bullet points for lists
   - Add transitions between sections

4. **Preserve All Key Information**: Don't omit important details, just reorganize them

5. **Maintain Accuracy**: Don't change technical facts or add information not in the source

Original User Query:
{user_query}

Execution Plan:
{plan_info}

Merged Results (may contain redundancy):
{merged_results}

Output a polished, professional final report with the following structure:
- Brief introduction referencing the user's query
- Numbered sections (1., 2., 3., etc.) following the execution plan's task descriptions
- Each section should clearly show which agent performed the analysis
- Clear, well-organized content with proper markdown formatting
- Brief conclusion or summary if appropriate

Use markdown formatting with clear headers and structure.
"""

def supervisor_node(state: State):
    """
    Supervisor: Plan ì‹¤í–‰ â†’ ê²°ê³¼ Merge â†’ ìµœì¢… ìš”ì•½
    """
    # ì „ì²˜ë¦¬ í™•ì¸
    if not state.get("preprocessed", False):
        print("\nâŒ ERROR: Preprocessing not complete")
        return Command(goto=END)
    
    plan = state.get('plan')
    
    if not plan:
        print("\nâŒ ERROR: No plan available")
        return Command(goto=END)
    
    print("\n" + "="*80)
    print(f"ğŸ® SUPERVISOR - Plan Execution (Iteration {state.get('current_iteration', 0) + 1})")
    print("="*80)
    
    # ë¬´í•œë£¨í”„ ë°©ì§€
    iteration = state.get('current_iteration', 0)
    if iteration >= MAX_SUPERVISOR_ITERATIONS:
        print("\nâš ï¸ Max iterations reached. Stopping execution.")
        return Command(goto=END)
    
    completed = state.get('completed_tasks', set())
    failed = state.get('failed_tasks', {})
    task_results = state.get('task_results', {})
    
    total_tasks = len(plan['tasks'])
    print(f"\nğŸ“Š Progress: {len(completed)}/{total_tasks} completed, {len(failed)} failed")
    
    if completed:
        print(f"   âœ… Completed: {', '.join(sorted(completed))}")
    if failed:
        print(f"   âŒ Failed: {', '.join(failed.keys())}")
    
    # ì‹¤í–‰ ê°€ëŠ¥í•œ task ì°¾ê¸°
    ready_tasks = get_ready_tasks(plan, completed, failed)
    
    # ===================================
    # Case 1: ì•„ì§ ì‹¤í–‰í•  taskê°€ ë‚¨ìŒ
    # ===================================
    if ready_tasks:
        print(f"\nğŸš€ Ready to execute: {len(ready_tasks)} task(s)")
        for task in ready_tasks:
            deps_str = ", ".join(task['depends_on']) if task['depends_on'] else "None"
            print(f"   â†’ [{task['task_id']}] {task['agent']} (deps: {deps_str})")
        
        # ìˆœì°¨ ì‹¤í–‰: ì²« ë²ˆì§¸ taskë§Œ ì‹¤í–‰
        task = ready_tasks[0]
        print(f"\nğŸ“Œ Executing task sequentially: {task['task_id']}")
        
        # ì´ì „ ì™„ë£Œëœ taskë“¤ì˜ ê²°ê³¼ë¥¼ contextë¡œ ì „ë‹¬
        previous_context = ""
        if completed:
            previous_context = "\n\n# Context from Previously Completed Tasks:\n"
            for prev_task_id in sorted(completed):
                if prev_task_id in task_results:
                    prev_task = next((t for t in plan['tasks'] if t['task_id'] == prev_task_id), None)
                    if prev_task:
                        previous_context += f"\n## [{prev_task_id}] {prev_task['description']}\n"
                        previous_context += task_results[prev_task_id] + "...\n"  # Summary for context
        
        task_state = {
            "messages": state["messages"],
            "patent_id": state["patent_id"],
            "preprocessed": state["preprocessed"],
            "plan": state["plan"],
            "task": task,
            "task_results": state.get("task_results", {}),
            "completed_tasks": state.get("completed_tasks", set()),
            "failed_tasks": state.get("failed_tasks", {}),
            "current_iteration": state.get("current_iteration", 0),
            "previous_context": previous_context  # ì´ì „ ê²°ê³¼ context
        }
        
        print(f"\nğŸ“¤ Dispatching task {task['task_id']} to {task['agent']}...")
        if completed:
            print(f"   ğŸ“ Including context from {len(completed)} previous task(s)")
        print("="*80)
        
        return Command(
            update={"current_iteration": iteration + 1},
            goto=Send(node=task['agent'], arg=task_state)
        )
    
    # ===================================
    # Case 2: ëª¨ë“  task ì™„ë£Œ - Merge & ìµœì¢… ìš”ì•½
    # ===================================
    if len(completed) + len(failed) == total_tasks:
        print("\nâœ… All tasks processed!")
        
        # ì‹¤íŒ¨í•œ taskê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ê·¸ëƒ¥ ì¢…ë£Œ
        if len(failed) > len(completed):
            print("\nâš ï¸ More failed than completed. Ending.")
            return Command(goto=END)
        
        # ê²°ê³¼ Merge
        merged_result = merge_task_results(plan, task_results, completed)
        
        # ìµœì¢… ìš”ì•½ ìˆ˜í–‰
        print("\n" + "-"*80)
        print("âœ… Performing final summarization...")
        print("-"*80)
        
        user_query = ""
        for msg in reversed(state["messages"]):
            if isinstance(msg, HumanMessage):
                user_query = msg.content
                break
        
        # Plan ì •ë³´ë¥¼ í¬ë§·íŒ…
        plan_info = f"Goal: {plan['goal']}\n\nTasks:\n"
        for i, task in enumerate(plan['tasks'], 1):
            plan_info += f"{i}. [{task['task_id']}] {task['description']} (Agent: {task['agent']})\n"
        
        summarization_messages = [
            SystemMessage(content=FINAL_SUMMARIZATION_PROMPT.format(
                user_query=user_query,
                plan_info=plan_info,
                merged_results=merged_result
            )),
            HumanMessage(content="Please create a clear, well-structured final report following the execution plan structure.")
        ]
        
        try:
            final_response = llm.invoke(summarization_messages)
            final_result = final_response.content if hasattr(final_response, 'content') else str(final_response)
            
            print("\nâœ“ Final summarization complete")
            print(f"âœ“ Final result length: {len(final_result)} characters")
            
        except Exception as e:
            print(f"\nâš ï¸ Error in final summarization: {e}")
            print("   Using merged result as-is")
            final_result = merged_result
        
        return Command(
            update={
                "messages": [AIMessage(content=final_result)],
                "merged_result": final_result
            },
            goto=END
        )




# =========================
# 12) Graph (ê¸°ì¡´ + Planner ì¶”ê°€)
# =========================
graph_builder = StateGraph(State)

# ë…¸ë“œ ì¶”ê°€
graph_builder.add_node("preprocess", preprocess_node)
graph_builder.add_node("planner", planner_node)  # NEW
graph_builder.add_node("supervisor", supervisor_node)
graph_builder.add_node("innovation_agent", innovation_node)
graph_builder.add_node("implementation_agent", implementation_node)
graph_builder.add_node("technical_agent", technical_node)
graph_builder.add_node("horizontal_agent", horizontal_node)

# ì—£ì§€ ì„¤ì •: START -> preprocess -> planner -> supervisor -> agents -> supervisor -> ...
graph_builder.add_edge(START, "preprocess")
# preprocessì—ì„œ plannerë¡œ (preprocess_nodeì—ì„œ Commandë¡œ ì§€ì •)
# plannerì—ì„œ supervisorë¡œ (planner_nodeì—ì„œ Commandë¡œ ì§€ì •)
# supervisorì—ì„œ agentsë¡œ (Send ì‚¬ìš©) ë˜ëŠ” ENDë¡œ
# agentsì—ì„œ supervisorë¡œ (_run_taskì—ì„œ Commandë¡œ ì§€ì •)

memory = MemorySaver()
graph = graph_builder.compile(checkpointer=memory)

print("âœ… Agent ë° Graph êµ¬ì„± ì™„ë£Œ (Planner í¬í•¨)")


# =========================
# Query Runner from Cell 5
# =========================
# =========================
# 13) Test Runner
# =========================
import nest_asyncio
import asyncio
nest_asyncio.apply()

CONFIG = {"configurable": {"thread_id": "1"}}

async def run_query(user_input: str, patent_id: str):
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        user_input: ì‚¬ìš©ì ì§ˆë¬¸ (í•œêµ­ì–´/ì˜ì–´)
        patent_id: íŠ¹í—ˆ ID (ì˜ˆ: US8526476)
    """
    init_state = {
        "messages": [HumanMessage(content=user_input)],
        "patent_id": patent_id,
        "preprocessed": False,
        "plan": None,
        "task_results": {},
        "completed_tasks": set(),
        "failed_tasks": {},
        "current_iteration": 0,
        "merged_result": "",
        "replan_feedback": "",
        "plan_iteration": 0,
        "next": ""
    }

    print("\n" + "#"*80)
    print("#" + " "*78 + "#")
    print("#  ğŸš€ ê³ ë„í™”ëœ LangGraph ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì‹¤í–‰" + " "*26 + "#")
    print("#" + " "*78 + "#")
    print("#"*80)
    print(f"\nğŸ’¬ User Query: {user_input}")
    print(f"ğŸ“„ Patent: {patent_id}")
    print("\n" + "="*80 + "\n")
    async for namespace, chunk in graph.astream(
        init_state,
        stream_mode="updates",
        subgraphs=True,
        config=CONFIG,
    ):
        for node_name, node_chunk in chunk.items():
            print(f"\n--- [{node_name}] ---")
            # node_chunkê°€ Noneì´ê±°ë‚˜ dictê°€ ì•„ë‹Œ ê²½ìš° ìŠ¤í‚µ
            if node_chunk is None or not isinstance(node_chunk, dict):
                print(f"(No update data)")
                continue
            
            if "messages" in node_chunk and node_chunk["messages"]:
                # ğŸ”¥ ìˆ˜ì •: ëª¨ë“  ë©”ì‹œì§€ ì¶œë ¥
                messages = node_chunk["messages"]
                
                # ToolMessageë“¤ì„ ì°¾ì•„ì„œ ëª¨ë‘ ì¶œë ¥
                from langchain_core.messages import ToolMessage
                tool_messages = [msg for msg in messages if isinstance(msg, ToolMessage)]
                
                if tool_messages:
                    print(f"\nğŸ“Š ì´ {len(tool_messages)}ê°œì˜ Tool í˜¸ì¶œ ê²°ê³¼:")
                    for i, tool_msg in enumerate(tool_messages, 1):
                        print(f"\n{'='*80}")
                        print(f"Tool Result #{i}")
                        print(f"{'='*80}")
                        try:
                            tool_msg.pretty_print()
                        except Exception:
                            print(tool_msg.content)
                else:
                    # Tool ë©”ì‹œì§€ê°€ ì•„ë‹Œ ê²½ìš° ë§ˆì§€ë§‰ ë©”ì‹œì§€ë§Œ ì¶œë ¥ (ê¸°ì¡´ ë¡œì§)
                    try:
                        messages[-1].pretty_print()
                    except Exception:
                        print(getattr(messages[-1], "content", messages[-1]))
            else:
                print(node_chunk)
    
    # ìµœì¢… ìƒíƒœ ë°˜í™˜
    final_state = await graph.aget_state(config=CONFIG)
    return final_state.values if final_state else init_state